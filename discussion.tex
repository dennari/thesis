Our aim in this thesis has been to explore the problem of static parameter estimation
in both linear-Gaussian and nonlinear-Gaussian SSMs. The chosen approach was to focus
on two methods for finding MAP/ML estimates, namely gradient based nonlinear optimization
and EM. Since the static
parameter estimation problem is tightly coupled with the filtering and smoothing problems, the
focus of the first part of the thesis was on state estimation. 
Nonlinear filtering and smoothing is
a considerable problem, where closed form solutions exist only in very few situations.
We advocated the Gaussian filtering approach and more specifically the cubature Kalman
filter and smoother. If the filtering and/or smoothing distributions are well approximated
by a Gaussian, these methods offer good approximate solutions for a fraction of
the computational complexity of the more general simulation based SMC methods.
 
The parameter estimation methods we have considered have specific strengths and weaknesses
which make them recommendable depending on the model. Let us go through
some of these and point out when they are evident in the two demonstrations of
Section~\ref{sec:results}.

\subsubsection*{Gradient based nonlinear optimization}

An important difference in the gradient based nonlinear optimization 
approach when compared to EM is that the smoothing distributions
are not needed. Neither does one need to figure out the model-dependent
M-step maximization equations. The marginal likelihood (Equation~\eqref{eq:logLH}), 
or an approximation to it if the model is nonlinear (Equation~\eqref{eq:nonl_logLH}), 
is obtained directly from the filtering algorithm.

A number of efficient gradient based nonlinear optimization
algorithms are available. We focused on the quasi--Newton BFGS algorithm,
which is implemented in \matlab's \texttt{fminunc}. Another BFGS implementation
is \texttt{ucminf} which has a \matlab{} version as well as a version for the open source
R software environment \parencite{Nielsen2000,rucminf2012,r2012}. 
As described in \textcite{Nielsen2000}, implementing a robust quasi--Newton method
is far from straightforward and if the objective is ML/MAP parameter estimation of SSMs,
it makes a lot of sense the utilize an off the shelf algorithm.
The main appeal of the gradient based
nonlinear optimization methods is their order of convergence, which can
be quadratic.  

The main issue is the score function computation. There are two alternative routes:
either the sensitivity equations, described in Section~\ref{sec:grad_LGSSM} for the linear
case and in Section~\ref{sec:grad_nonlinear} for the nonlinear, or using
Fisher's identity and the EM machinery as described in Section~\ref{sec:fisheri}.
Using the sensitivity equations leads to recursive equations which need
to be run alongside and tightly coupled to the chosen filtering algorithm.
This is an issue from the perspective of being able to use decoupled modular
algorithms. Moreover, the Jacobian matrices of $\ff$ and $\hh$ are required.
As for the computational complexity, the sensitivity equations scale
as $d_\theta$ Kalman filters \parencite{Cappe2005, Olsson2007}.
A Kalman filter scales as $\mathcal{O}(n^3)$. 

With Fisher's identity the parameter estimation algorithm can be made 
less coupled to the filtering algorithm, but then a separate smoothing
step is required. This option doesn't require the Jacobians and is
is better suited for modular implementation. Moreover, since
the smoothing algorithm can be considered to have the same computational
complexity as a Kalman filter, using Fisher's identity has approximately
the computational complexity of two Kalman filters.

  
\subsubsection*{EM}
  
Using Fisher's identity as part of a nonlinear gradient based optimization method 
can be considered to be some sort of hybrid approach between the sensitivity equations
and the full EM solution. Thus some of the strengths and weaknesses that were
mentioned in the previous chapter when comparing sensitivity equations and Fisher's identity
apply directly when comparing the sensitivity equations to the EM solution.

A critical question in this comparison is whether the M-step maximization equations
can be computed in closed form. If this is not the case and the M-step includes
some sort of gradient based nonlinear optimization in itself as part of a generalized
EM (gEM) method (mentioned in section~\ref{sec:EM_partial}) using EM certainly loses
some of its appeal. 
%In fact we assume that in this case one would choose to apply
%gradient based nonlinear optimization instead and so in the sequel it is assumed
%that the M-step equations can be solved in closed form.

When the M-step maximization equations
can be computed in closed form, EM gains some strengths.
First and foremost, no gradient computations are needed.
As a consequence, in this case EM is also independent
of the parameterization, since the M-step consists only of
maximization operations \parencite{Cappe2005}. This is not true
for the gradient based methods.

As for the order of convergence of EM, there exists some rather interesting results
which are discussed at least in 
\textcite{Salakhutdinov2003,Salakhutdinov2004},
\textcite{Petersen2005a}, and \textcite{Gibson2005}. As a summary,
it seems that the convergence properties depend on the proportion 
of the total information that is contained in the latent variables. Intuitively,
the larger this proportion is, the slower is the convergence. Also, if this proportion
is small, the order of convergence can approach that of a true Newton's method (i.e. quadratic).
Interesting results concerning the convergence properties of EM (in linear models) were also
obtained in \textcite{Petersen2005a} and \textcite{Petersen2005}. Their analyses seem
to show that the order of convergence depends on the signal-to-noise ratio (SNR) in
such as a way that the convergence slows down when SNR becomes high.
Strategies for speeding up the convergence of EM has been
a subject of much interest and some approaches are presented at least
in \textcite{Meng1997} and \textcite{Lange1995}.

In the nonlinear case, one has to resort to approximate filtering and smoothing.
This means that the marginal log-likelihood and score function computations 
become approximations.
At least with Gaussian filtering and smoothing, these approximations seem to be different between
the sensitivity equations and EM (or Fisher's identity). This can be deduced from the fact
that when computing the score function through Fisher's identity or the maximization equations of
the M-step in EM, we need the quantity $\v{P}_{k-1,k}$ in
Equation~\eqref{eq:joint_predictive_approximation}. Since $\v{P}_{k-1,k}$ is part
of the Gaussian approximation and it is not needed when approximating the score function
through the sensitivity equations, the score function approximations
are most probably unequal.  

The unequality of the approximations appears to be demonstrated in
the results of the two demonstrations in Section~\ref{sec:results}. 
With the linear-Gaussian SSM of Section~\ref{sec:ballistic} the two methods give identical results 
(when attributing the tiny differences
to differing numerical properties). 
However with the nonlinear-Gaussian SSM 
of Section~\ref{sec:harmonic} the results
were markedly different, even though both methods were given the same data and initial estimates.
Deriving the quantitative difference in the approximations would be an interesting subject
for future work.



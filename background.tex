\subsection{State space models}

State space models (SSMs) provide a unified probabilistic methodology for modeling
sequential data \parencite{ljung1994modeling,durbin2012time,Cappe2005,barber2011bayesian}. Sequential data arise in numerous applications, typically in the form
of time-series measurements \todo{examples}. However it is not necessary for the sequence index to have
a temporal meaning. In probabilistic terms a time-series
can be described by a \emph{stochastic process} $\z = \left\{\z_k:k\in K\right\}$, where $\z_k$ is a random
variable and $K\subset\R$ for continuous time or $K\subset\field{N}$ for discrete time sequences. 
In this thesis we will only be concerned with discerete time processes and the sample space of $\z_k$ will be $\R^d$. 
The shorthand $\z_{1:k}$ will be used to mean the subset $\{\z_1,\dots,\z_k\}$. We will also
denote by $\v{Z}$ the $d\times T$ matrix, that has all $T$ values of the process $\z$ as columns.

A fundamental question in probabilistic models for sequential data is how 
to model the dependence between variables. It is infeasible to assume
that every random variable in the process depends on all the others.
Thus it is common to assume a \emph{Markov chain}, where the distribution of
the process at the current timestep depends only on the distribution in the previous timestep.
A further assumption in SSMs is that the process of interest, the dynamic process $\x$, is not directly observed
but only through another stochastic process, the \emph{measurement process} $\y$. Since
$\x$ is not observed, SSMs belong to the class of \emph{latent variable models}. Sometimes, as in
\cite{Cappe2005}, SSMs are called \emph{hidden Markov models} (HMM) but usually this implies that
the sample space of $\x$ is discrete. Another assumption is that the values of the measurement process are conditionally independent
given the latent Markov process.
An intuitive way to present conditional independence properties between random
variables is a \emph{Bayes network} presented by a directed acyclic graph (DAG) \parencite{pearl1988probabilistic,Bishop2006}.
%The conditional independence properties of a discrete-time SSM are presented in figure~\ref{fig:ssm_graphical}.
A Bayes network presentation of a discrete-time SSM is given in figure~\ref{fig:ssm_graphical}.

\begin{figure}[!htp]
	\centering
	\begin{tikzpicture}
	\tikzstyle{main}=[circle, minimum size = 5mm, inner sep=0mm, thick, draw =black!80, node distance = 12mm,font=\small]
	\tikzstyle{param}=[circle, minimum size = 2mm, inner sep=0mm, thick, draw =white!80, fill=black!80, node distance = 18mm]
	\tikzstyle{ellipsis}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 18mm]
	\tikzstyle{connect}=[-latex, semithick]
	  %\node[main,node distance=50mm] (theta) [label=above:$\theta$] {};
	  \node[main] (x_1) [label=above:$\v{x}_{k-1}$] {};
	  \node[main,draw=white!0] (prev) [left of=x_1] {$\dots$};
	  \node[main] (x) [right of=x_1,label=above:$\v{x}_{k}$] {};
	  \node[main] (x__1) [right of=x,label=above:$\v{x}_{k+1}$] {};
	  \node[main,fill = black!10] (y_1) [below of=x_1,label=below:$\v{y}_{k-1}$] {};
	  \node[main,fill = black!10] (y) [below of=x,label=below:$\v{y}_{k}$] {};
	  \node[main,fill = black!10] (y__1) [below of=x__1,label=below:$\v{y}_{k+1}$] {};
	  \node[main,draw=white!0] (next) [right of=x__1] {$\dots$};
	  \path
	    (prev) edge [connect] (x_1)
	    (x_1) edge [connect] (x) 
	    	  edge [connect] (y_1)
	    (x) edge [connect] (y)
	    	edge [connect] (x__1)
	    (x__1) edge [connect] (y__1)
	    	edge [connect] (next);
	\end{tikzpicture}
	\caption{SSM as a graphical model presented with a directed acyclic graph}
	\label{fig:ssm_graphical}
\end{figure}
The value $\xk \in \mathcal{X} \subset \R^{d_x}$ of the dynamic process at time $k$ is called the
\emph{state}\todo{Explain state} at time $k$. For the measurements we define $\yk \in \mathcal{Y} \subset \R^{d_y}$.  
Taking into account the Markov property 
\begin{align}
	\Pdf{\xk}{\x_{1:k-1}}&=\Pdf{\xk}{\x_{k-1}}
\end{align}
of the dynamic process and the conditional
independence property 
\begin{align}
	\Pdf{\yk}{\x_{1:k},\y_{1:k-1}}&=\Pdf{\yk}{\xk}
\end{align}
of the measurement process, the joint distribution of states
and measurements factorises as
\begin{align}
	\Pdf{\X,\Y}{\Th}&=\Pdf{\v{x}_0}{\Th}\prod_{k=1}^T\Pdf{\v{x}_k}{\v{x}_{k-1},\Th}\Pdf{\v{y}_k}{\v{x}_{k},\Th}
	\label{eq:complete_data_likelihood}.
\end{align}
Thus in order to describe a SSM one needs to specify three probability distributions:
\begin{description}
\addtolength{\leftskip}{1cm}
	\item[Prior distribution]
	$\Pdf{\v{x}_0}{\Th}$ is the distribution assumed for the state prior to observing any measurements. The
	sensitivity of the posterior distributions to the prior depends on the amount of data (the more data the less sensitivity).
	\item[Dynamic model]
	$\Pdf{\v{x}_k}{\v{x}_{k-1},\Th}$ dictates the time evolution of the states
	\item[Measurement model]
	$\Pdf{\v{y}_k}{\v{x}_{k},\Th}$ models how the observations depend on the state and the statistics of the noise
\end{description}
In this thesis it is assumed that the parametric form of these distributions is known
for example by physical modeling \parencite{ljung1994modeling}. However the distributions are dependent on
the vector parameter $\Th \in \Theta \subset \R^{d_\theta}$ whose value (or distribution) we would like to learn from
the measurements.

Traditionally SSMs are specified as a pair of equations specifying the dynamic and measurement models. The most general
presentation of discrete-time SSMs is 
\begin{subequations}
\label{eq:ssm_too_general}
\begin{align}
	\xk &= \v{f}_{\Th,k}\left(\x_{k-1},\v{q}_{k-1}\right)\\
	\yk &= \v{h}_{\Th,k}\left(\xk,\v{r}_{k}\right).
\end{align}
\end{subequations}
Here the stochasticity is separated into the noise processes $\v{q}$ and $\v{r}$ which are usually
assumed to be zero mean, white and independent of each other. We will restrict ourselves
to the case of zero mean, white and additive Gaussian noise and in our case the mappings
$\v{f}_{\Th,k}$ and $\v{h}_{\Th,k}$ and the noise processes $\v{q}$ and $\v{r}$ will be stationary (i.e. independent of
k). Thus the SSMs considered in this thesis are of the form
\begin{subequations}
\label{eq:ssm_general}
\begin{alignat}{2}
	\xk &= \v{f}_{\Th}\left(\x_{k-1}\right)+\v{q}_{k-1}&\v{q}_k&\sim \N{\v{0},\v{Q}} \label{eq:Q}\\
	\yk &= \v{h}_{\Th}\left(\xk\right)+\v{r}_{k}&\v{r}_k&\sim \N{\v{0},\v{R}} \label{eq:R}\\
	\v{x}_{0} &\sim \N{\gv{\mu},\gv{\Sigma}}& \label{eq:prior}
\end{alignat}
\end{subequations}
We will assume an implicit dependence of the distributional parameters $\v{Q}$,
$\v{R}$,$\gv{\mu}$ and $\gv{\Sigma}$ on $\Th$. 
Now clearly the mappings $\v{f}_{\Th}:\mathcal{X}\to\mathcal{X}$ and
$\v{h}_{\Th}:\mathcal{X}\to\mathcal{Y}$ specify the means of the dynamic
and the measurement models:
\begin{subequations}
\label{eq:ssm_distr_general}
\begin{align}
	\Pdf{\xk}{\x_{k-1},\Th}&=\N{\v{f}_{\Th}\left(\x_{k-1}\right),\v{Q}}\label{eq:dynamics_distr_general}\\
	\Pdf{\yk}{\xk,\Th}&=\N{\v{h}_{\Th}\left(\xk\right),\v{R}}\label{eq:measurement_distr_general}
\end{align}
\end{subequations}

\subsubsection*{Example: 1D random walk}
The simplest example is a one dimensional random-walk observed in Gaussian noise.
We will assume $\Pdf{\v{x}_0}=\N{0,P_0}$. In an alternative (but equivalent) notation
the dynamics model is now
\begin{align}
	\Pdf{x_k}{x_{k-1}}&=x_{k-1}+q_{k-1},
\end{align}
where $q_{k-1}\sim \N{0,Q}$ and the measurement model is
\begin{align}
	\Pdf{y_k}{x_{k}}&=x_{k}+r_{k},
\end{align}
where $r_{k}\sim \N{0,R}$. A simulation from the model is presented in figure~\ref{fig:rw1d}.

\begin{figure}[htp]
\begin{center}
	\missingfigure{A missing 1D RW simulation}
  \caption{Simulation from the 1D RW model}
  \label{fig:rw1d}
\end{center}
\end{figure}


\subsection{Bayesian optimal filtering and smoothing}

Inference can be defined as answering questions of interest with a probability distribution \parencite{barber2011bayesian}.
In case of SSMs there are many questions of interest, but most commonly one would
like to know the \emph{marginal posterior distribution} of the states. State inference
can be divided into subcategories based on the temporal relationship between the state
and the observations \parencite{Sarkka2006}:
\begin{description}
\addtolength{\leftskip}{1cm}
	\item[Predictive distribution]
	$\Pdf{\v{x}_{k}}{\v{y}_{1:k-1}}$ is the predicted distribution of the state in the next timestep (or more generally at timestep $k+h$, where $h>0$) 
	given the previous measurements
	\item[Filtering distribution] $\Pdf{\v{x}_k}{\v{y}_{1:k}}$ is the marginal posterior distribution
	of any state $\xk$ given the measurements up to and including $\yk$
	\item[Smoothing distribution]
	$\Pdf{\v{x}_k}{\v{y}_{1:T}}$ is the marginal posterior distribution
	of any state $\xk$ given the measurements up to and including $\y_T$ where $k<T$
\end{description} 

\subsubsection*{Predictive distribution}
Let us now derive a recursive formulation for computing the filtering distribution at time $k$. Let $\Pdf{\xkk}{\y_{1:k-1}}$
be the filtering distribution of the previous step. Then 
\begin{align}
	\Pdf{\xk}{\y_{1:k-1}}&=\defint{}{}{\Pdf{\xk,\xkk}{\y_{1:k-1}}}{\xkk} \nonumber\\
	&=\defint{}{}{\Pdf{\xk}{\xkk}\Pdf{\xkk}{\y_{1:k-1}}}{\xkk},
	\label{eq:pred_bayes}
\end{align}
which is known as the \emph{Chapman-Kolmogorov equation} \parencite{Sarkka2006}.
\subsubsection*{Filtering distribution}
Incorporating the newest measurement can be achieved with the Bayes'
rule (see for example \cite{gelman2004})\todo{check marginal likelihood wording}
\begin{align}
	\underbrace{\Pdf{\xk}{\y_{1:k}}}_\text{posterior}&=\frac{\overbrace{\Pdf{\yk}{\xk}}^\text{likelihood}\overbrace{\Pdf{\xk}{\y_{1:k-1}}}^\text{prior}}{\underbrace{\Pdf{\y_{k}}{\y_{1:k-1}}}_\text{marginal likelihood}}\nonumber\\
	&=\frac{\Pdf{\yk}{\xk}\Pdf{\xk}{\y_{1:k-1}}}{\defint{}{}{\Pdf{\yk}{\xk}\Pdf{\xk}{\y_{1:k-1}}}{\xk}}
\end{align}
which is called the measurement update equation.

\subsubsection*{Smoothing distribution}
The smoothing distributions can also be computed recursively by assuming that the filtering distributions
and the smoothing distribution $\Pdf{\x_{k+1}}{\y_{1:T}}$ of the ``previous'' step are available.
Since
\begin{align*}
	\Pdf{\xk}{\x_{k+1},\y_{1:T}}&=\Pdf{\xk}{\x_{k+1},\y_{1:k}}\\
	&=\frac{\Pdf{\xk,\x_{k+1}}{\y_{1:k}}}{\Pdf{\x_{k+1}}{\y_{1:k}}}\\
	&=\frac{\Pdf{\x_{k+1}}{\xk}\Pdf{\xk}{\y_{1:k}}}{\Pdf{\x_{k+1}}{\y_{1:k}}}
\end{align*}
we get
\begin{align}
	\Pdf{\xk}{\y_{1:T}}&=\Pdf{\xk}{\y_{1:k}}\defint{}{}{\left[\frac{\Pdf{\x_{k+1}}{\xk}\Pdf{\x_{k+1}}{\y_{1:T}}}{\Pdf{\x_{k+1}}{\y_{1:k}}}\right]}{\x_{k+1}},
\end{align}
where $\Pdf{\x_{k+1}}{\y_{1:k}}$ can be computed by equation \eqref{eq:pred_bayes}.

\subsubsection*{Marginal likelihood}

An important quantity concerning parameter estimation is the marginal likelihood $\Pdf{\y_{1:T}}$. Since
\begin{align}
	\Pdf{\y_k}{\y_{1:k-1}}&=\defint{}{}{\Pdf{\yk}{\xk}\Pdf{\xk}{\y_{1:k-1}}}{\xk}
\end{align}
the marginal likelihood can be computed from
\begin{align}
	\Pdf{\y_{1:T}}&=\Pdf{\y_1}\prod_{k=2}^T \Pdf{\y_k}{\y_{1:k-1}}
\end{align}



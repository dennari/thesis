
As mentioned in the introduction, it is often the case that
after constructing a SSM, the result is actually a family of models,
indexed by the static parameter $\Th$. Commonly, but not necessarily, the interest
is in the states and we need to specify some ``good' value for $\Th$ before
filtering or smoothing. In general, parameter estimation techniques are divided
into \emph{offline} or \emph{batch} methods and \emph{online} or \emph{recursive} methods
\parencite{Cappe2007}. 
This is analogous to the filtering and smoothing problems in state estimation.
We focus only on offline methods, where some sort of training or calibration data has been
acquired beforehand.

Let us then concern ourselves for a moment with the \emph{difference} between the states
and the parameters in a SSM.
The need for acknowledging the difference is obvious when noting that the SSM of equation~\eqref{eq:ssm_too_general}
could be reformulated by augmenting $\Th$ as part of the state and
modifying $\ff$ and $\hh$ accordingly. In this
way a separate parameter estimation problem would not exist. However, the 
separation becomes important after assuming the parameters \emph{static}. Instead of 
approximating a separate ``parameter" distribution for every
timestep, we then know that by separating the static part into
$\Th$ we can do with a single parameter distribution for the whole model. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian Estimation of Parameters}%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We now shift the focus on estimating the parameter $\Th$, which was assumed
given in the case of state estimation. In the Bayesian sense the complete 
answer would be the \emph{joint} posterior distribution of the states and the parameters given the data 
\begin{align}
	% \Pdf{\X,\Th}{\Y}\propto\frac{\Pdf{\X,\Y}{\Th}}{\Pdf{\Y}{\Th}}\frac{\Pdf{\Y}{\Th}\Pdf{\Th}}{\defint{\Theta}{}{\Pdf{\Y}{\Th}\Pdf{\Th}}{\Th}}
	\Pdf{\X,\Th}{\Y} &\propto \Pdf{\X,\Y}{\Th}\Pdf{\Th}\\
	&= \Pdf{\X}{\Y,\Th}\Pdf{\Y}{\Th}\Pdf{\Th}
	\label{eq:param_post}
\end{align}
By defining the SSM in equation~\eqref{eq:ssm_general}, we have
implicitely defined the ``complete-data'' likelihood $\Pdf{\X,\Y}{\Th}$
(see Equation~\eqref{eq:complete_data_likelihood}).
By introducing the \emph{prior distribution}, $\Pdf{\Th}$,
the components of \eqref{eq:param_post} and thus the joint distribution
of states and parametes is defined. Recently, methods known as
\emph{Particle Markov chain Monte Carlo} (PMCMC) have emerged,
which are able to sample from the joint distribution in \eqref{eq:param_post}
\parencite{Andrieu2010}.
This is achieved by combining particle filtering approximations to $\Pdf{\X}{\Y,\Th}$ 
with traditional Gibbs and Metropolis-Hastings sampling in a nontrivial way \parencite{gelman2004}.

\subsubsection{Maximum a posteriori and maximum likelihood}

In this thesis we would like to avoid Monte Carlo methods altogether. Thus instead
of considering the problem of finding the posterior distribution of the parameter,
we will pursue finding the mode of this distribution, i.e. the \emph{maximum a posteriori} (MAP) estimate
$\Th_{\text{MAP}}$. The MAP estimate is not necessarily unique. 
We will consider the uniqueness issue later in more detail, but in order
to not get distracted, let us assume for the moment that the posterior distribution in fact
has a unique maximum. Since the logarithm is a strictly monotonic function, maximizing a function
is the same as maximizing its logarithm. 
Since $\Y$ is observed, let us denote 
the log marginal likelihood with 
\begin{align*}
	\lLH \equiv \log \Pdf{\Y}{\Th}.
\end{align*}
The MAP estimate of $\Th$ is then defined as 
\begin{align}
	\Th_{\text{MAP}} &\equiv \argmax_{\Th}\brak[\big]{ \log\Pdf{\Th}{\Y}} \nonumber\\ 
	&=\argmax_{\Th}\brak[\big]{\lLH + \log\Pdf{\Th}} \nonumber\\
	&=\argmin_{\Th}\brak[\big]{\ene},
	\label{eq:MAP}
\end{align}
where we define the \emph{energy function}\todo{should this be introduced?}
\parencite{Mbalawataa}
\begin{align}
	\ene \equiv -\lLH - \log\Pdf{\Th}.
	\label{eq:ene}
\end{align}
The MAP estimate might be used as is in order to obtain a state estimates
with supposedly probable parameter values. It could also be used as the initial
value for simulation based approaches or to compare results obtained with other
methods.

In the case of a uniform (constant and thus improper)
prior distribution, $\Pdf{\Th}=C$, the MAP estimate reduces to the
\emph{maximum likelihood} (ML) estimate
\begin{align}
	\Th_{\text{ML}} &\equiv \argmax_{\Th}\brak[\big]\lLH\label{eq:ML} \\
	&= \argmax_{\Th}\brak[\big]{\lLH+\log C} \nonumber.
\end{align}
In the limit of infinite data, the influence of the prior
disappears. Then if the support of the prior includes the true
parameter value, the MAP estimator has the same asymptotic properties
as the ML estimate \parencite{Cappe2005}. These properties will be discussed in more
detail in the sequel. 

With the help of the Gaussian filtering and smoothing methodology introduced in the
previous section, computing the (approximate) MAP estimate corresponds to maximizing a
completely known function. Thus the problem is turned into one of nonlinear optimization (also called
nonlinear programming) \parencite{Cappe2005}.

\subsubsection{Ascent methods}

Both of the parameter estimation methods we are going
discuss, the expectation maximization algorithm and
the instances of gradient based nonlinear programming dealt with in the
next chapter, belong to the class of \emph{iterative ascent methods} \parencite{luenberger2008}.
Suppose that $\v{m}:\Theta \to \Theta$ defines an iterative ascent method
and that we are maximizing the objective function $\varphi:\Theta\to\R$.
Then given some initial point $\Th_0$, the sequence of estimates
$\brac{\Th_j \in \Theta: \Th_j=\v{m}(\Th_{j-1})}$ where $j=1,\dots$
has the property $\varphi(\Th_{j})\geq \varphi(\Th_{j-1})$. This means
that the objective function is increased at every iteration of
an iterative ascent method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient based nonlinear optimization of parameters}\label{sec:grad}%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{parameter_est_gradient}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Expectation maximization (EM)}%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{parameter_est_em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Theoretical considerations}%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{parameter_est_theoretical}




As mentioned in the introduction, it is usually the case that
after constructing a SSM, the result is a family of models
indexed by the static parameter $\Th$. The ultimate interest might lie in estimating
the states or the parameter or both. Be as it may, the two inference problems are intimately
coupled and interest in the other requires the resolution of the other.

In general, parameter estimation techniques are divided
into offline or \emph{batch} methods and online or \emph{recursive} methods
\parencite{Cappe2007,Kantas2009}. This is analogous to the difference between the filtering and 
smoothing problems in state estimation. We focus only on offline methods, where some 
sort of training or calibration data has been acquired beforehand.

A classic solution to the parameter estimation problem is to introduce
an augmented and necessarily nonlinear SSM, where the parameters have been concatenated as part of the
state. For static parameters, the part of the dynamic model corresponding to the parameters is
set to identity. Classically an extended Kalman filter is 
then applied to approximate the probability distribution of the augmented state
vector in the joint space of parameters and states. This approach is known
as \emph{joint EKF} and it has the virtue of being an online procedure \parencite{Wan2001}.
It appears that the method has problems with convergence in some situations.
A more recent method utilizing another form of augmented SSM is known as \emph{iterated filtering}
\parencite{Ionides2011}. It is an offline method, but only requires being able
to sample from the dynamic model given the parameter and no gradient computations
are required. The algorithm however introduces multiple parameters of its own
and so might require some tuning \parencite{Kantas2009}. Furthermore, it is designed
to utilize the simulation based SMC methods mentioned briefly in Section~\ref{sec:nonlinear_state}.
In the sequel, these kinds of filtering based parameter estimation methods 
will not be further considered. 


% Let us then concern ourselves for a moment with the difference between the states
% and the parameters in a SSM. In the introduction it was stated that in this
% thesis the distinction is that the states are dynamic and the parameters are
% static. But this is only true in the sense that for the states a probability
% distribution is computed for every timestep. The marginal distribution
% of the static components could still be static.
% Thus the SSM of equation~\eqref{eq:ssm_too_general}
% could be reformulated by augmenting $\Th$ as part of the state and
% modifying $\ff$ and $\hh$ accordingly. In this
% way a separate parameter estimation problem would not exist.
% Parameter estimation by state augmentation is known as  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian Estimation of Parameters}%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the Bayesian sense the complete 
answer to the filtering and parameter estimation problems would be the \emph{joint} posterior distribution of the states
and the parameters given the data
\begin{align}
	% \Pdf{\X,\Th}{\Y}\propto\frac{\Pdf{\X,\Y}{\Th}}{\Pdf{\Y}{\Th}}\frac{\Pdf{\Y}{\Th}\Pdf{\Th}}{\defint{\Theta}{}{\Pdf{\Y}{\Th}\Pdf{\Th}}{\Th}}
	\Pdf{\X,\Th}{\Y} &\propto \Pdf{\X,\Y}{\Th}\Pdf{\Th}\\
	&= \Pdf{\X}{\Y,\Th}\Pdf{\Y}{\Th}\Pdf{\Th}
	\label{eq:param_post}
\end{align}
By defining the SSM in equation~\eqref{eq:ssm_general}, we have
implicitly defined the ``complete-data'' likelihood $\Pdf{\X,\Y}{\Th}$
(see Equation~\eqref{eq:complete_data_likelihood}).
By introducing the \emph{prior distribution}, $\Pdf{\Th}$,
the components of \eqref{eq:param_post} and thus the joint distribution posterior
of states and parametes is defined. Recently, methods known as
\emph{Particle Markov chain Monte Carlo} (PMCMC) have emerged,
which are able to sample from the joint distribution in \eqref{eq:param_post}
without knowledge of the normalization constant \parencite{Andrieu2010}.
This is achieved by combining particle filtering approximations to $\Pdf{\X}{\Y,\Th}$ 
with traditional Gibbs and Metropolis-Hastings sampling in a nontrivial way 
\parencite{Andrieu2010,gelman2004}.

\subsubsection{Maximum a posteriori and maximum likelihood}

In this thesis we would like to avoid Monte Carlo methods altogether. Thus instead
of considering the problem of finding the posterior distribution of the parameter,
we will pursue finding the mode of this distribution, i.e. the \emph{maximum a posteriori} (MAP) estimate
$\Th_{\text{MAP}}$. The MAP estimate is not necessarily unique. 
We will consider the uniqueness issue later in more detail, but in order
to not get distracted, let us assume for the moment that the posterior distribution in fact
has a unique maximum. Since the logarithm is a strictly monotonic function, maximizing a function
is the same as maximizing its logarithm. 
Since $\Y$ is observed, let us denote 
the log marginal likelihood with 
\begin{align*}
	\lLH \equiv \log \Pdf{\Y}{\Th}.
\end{align*}
The MAP estimate of $\Th$ is then defined as 
\begin{align}
	\Th_{\text{MAP}} &\equiv \argmax_{\Th}\brak[\big]{ \log\Pdf{\Th}{\Y}} \nonumber\\ 
	&=\argmax_{\Th}\brak[\big]{\lLH + \log\Pdf{\Th}+C}, && \text{($C$ is independent of $\Th$)} \nonumber\\
	&=\argmax_{\Th}\brak[\big]{\lLH + \log\Pdf{\Th}}\\
	%&=\argmin_{\Th}\brak[\big]{\ene},
	\label{eq:MAP}
\end{align}
\todo{explain the uses of the MAP estimate from Gelman}
%The MAP estimate might be used as is in order to obtain a state estimates
%with supposedly probable parameter values. It could also be used as the initial
%value for simulation based approaches or to compare results obtained with other
%methods.

In the case of a uniform (constant and thus improper)
prior distribution, $\Pdf{\Th}=C$, the MAP estimate reduces to the
\emph{maximum likelihood} (ML) estimate
\begin{align}
	\Th_{\text{ML}} &\equiv \argmax_{\Th}\brak[\big]\lLH\label{eq:ML}
\end{align}
In the limit of infinite data, the influence of the prior
disappears. Then if the support of the prior includes the true
parameter value, the MAP estimate has the same asymptotic properties
as the ML estimate \parencite{Cappe2005}. These properties will be discussed in more
detail in Section~\ref{sec:theory}. Since the mathematical difference
between the MAP and ML estimates depends only on the model dependent prior distribution
assigned to $\Th$, we will mainly focus on computing the ML estimate.
Some steps where the prior plays an important role will be separately
highlighted. 

With the help of the Gaussian filtering and smoothing methodology introduced in 
Section~\ref{sec:nonlinear_state}, computing the (approximate) MAP estimate 
corresponds to maximizing a completely known function.
Thus the problem is turned into one of nonlinear optimization 
(also called nonlinear programming) \parencite{Cappe2005}.

\subsubsection{Ascent methods}

Both of the parameter estimation methods we are going
discuss, the expectation maximization algorithm and
the instances of gradient based nonlinear programming dealt with in the
next chapter, belong to the class of \emph{iterative ascent methods} \parencite{luenberger2008}.
Suppose that $\v{m}:\Theta \to \Theta$ defines an iterative ascent method
and that we are maximizing the objective function $\varphi:\Theta\to\R$.
Then given some initial point $\Th_0$, the sequence of estimates
$\brac{\Th_j \in \Theta: \Th_j=\v{m}(\Th_{j-1})}$ where $j=1,\dots$
has the property $\varphi(\Th_{j})\geq \varphi(\Th_{j-1})$. This means
that the objective function is increased at every iteration of
an iterative ascent method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient based nonlinear optimization of parameters}\label{sec:grad}%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{parameter_est_gradient}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Expectation maximization (EM)}\label{sec:em}%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{parameter_est_em}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Theoretical considerations}\label{sec:theory}%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \input{parameter_est_theoretical}




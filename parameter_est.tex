%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Maximum likelihood and maximum a posteriori estimation}%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the Bayesian sense the complete answer to the parameter estimation
problem is the marginal posterior probability of the parameters
given the measurements, which is given by Bayes' rule as
\begin{align}
	\Pdf{\gv{\theta}}{\Y}&=\frac{\Pdf{\Y}{\gv{\theta}}\Pdf{\gv{\theta}}}{\Pdf{\Y}}.
	\label{eq:param_post}
\end{align}

Computing the posterior distribution of the parameters is usually intractable \todo{why? examples?}. A much
easier problem is finding a suitable \emph{point estimate} $\hat{\gv{\theta}}$.
This effectively means that we don't need to worry about the normalizing
term $\Pdf{\Y}$, since it's constant with respect to the parameters. 
A point estimate that maximizes the posterior distribution
is called a \emph{maximum a posteriori} (MAP) estimate. 
Since the logarithm is a strictly monotonic function, maximizing a function
is the same as maximizing its logarithm. Thus the MAP estimate $\Th^*$ is given by 
\begin{align}
	\Th_{\text{MAP}} &= \text{argmax}_{\Th}\left[\underbrace{\log \Pdf{\Y}{\gv{\theta}}}_{\Pdf[\ell][2pt]{\Th}} + \log\Pdf{\gv{\theta}}\right]
	\label{eq:MAP}
\end{align}

In the case of a flat (constant and thus improper)
prior distribution, $\Pdf{\Th}$, the MAP estimate converges to the
\emph{maximum likelihood} (ML) estimate
\begin{align}
	\Th_{\text{ML}} &= \text{argmax}_{\Th}\left[\lLH\right]
	\label{eq:ML}
\end{align}
Going further we we will only be concerned with finding the ML estimate, but it should
be remembered that both of the methods we consider can be extended
to the estimation of the MAP estimate in a straightforward fashion.
\todo{Explain MAP in both cases}

Among different point estimates, the maximum likelihood estimator has good statistical properties.
Let us denote the true parameter value, the value that the data was generated with, with $\Th_\star$ and 
let $T$ denote the amount of observations.
Then provided that some conditions of not very restricting nature hold, we can state the following asymptotic properties 
for the ML estimate $\Th_{\text{ML}}$:\todo{Modify to reflect p.465 in Cappé}
\begin{description}
\addtolength{\leftskip}{1cm}
\item[Strong consistency]\hfill\\
An important property for an estimator, which says that
the estimator tends to the true value as the amount of data tends to infinity:
\begin{align}
	%\forall \Th \in \Theta\quad \frac{1}{n}\Pdf[\ell_n]{\Th} \xrightarrow{\mathrm{a.s.}} \lLH, \mathrm{when} n\to\infty
	\ell_T\left(\Th_{\text{ML}}\right) \xrightarrow{\mathrm{a.s.}} \ell\left(\Th_\star\right),\quad \mathrm{when}\;T\to\infty,
\end{align}
where $\ell_T$ is the likelihood function after $T$ measurements and $\ell$ is a continuous
deterministic function with a unique global maximum at $\Th_\star$.
%where $\Pdf[\ell_n]{\Th}$ is the log-likelihood given $n$ observations and $\lLH$ is a continuous deterministic
%function with a unique global maximum at $\Th_\star$.
\item[Asymptotic normality]\hfill\\
This property gives us the means to compute asymptotic error bounds for
the estimate:
\begin{align}
	\sqrt{T}\left(\Th_{\text{ML}}-\Th_\star\right) \xrightarrow{D} \N{\v{0},\mathcal{I}^{-1}\left(\Th_\star\right)},\quad \mathrm{when}\;T\to\infty,
	\label{tablelabel}
\end{align}
where $\mathcal{I}\left(\Th_\star\right)$ is the \emph{Fischer information matrix} evaluated at $\Th_\star$ 
\item[Efficiency]\hfill\\
When the amount of information tends to infinity, the ML-estimate achieves
the Cramér-Rao lower bound, i.e. no other consistent estimator has lower asymptotic mean-squared-error.
\end{description}


\subsubsection{Identifiability}

Intuitively, any parameters $\Th,\:\Th' \in \Theta$ cannot be distinguished
from each other with maximum likelihood estimation if
\begin{align}
	\Pdf{\Y}{\Th}&=\Pdf{\Y}{\Th'},
\end{align}
i.e., if the same data can arise with two (or more) separate
parameter values.
\todo{Elaborate on identifiability}
\parencite{Haykin2001,Cappe2005}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient based nonlinear optimization}\label{sec:grad}%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This is the classical way of solving the parameter estimation problem. It consists
of computing the gradient of the log-likelihood function $\lLH$ and then using some
non-linear optimization method to find a \emph{local} maximum to it 
\parencite{Mbalawataa,Cappe2005}. 
An efficient non-linear optimization algorithm is the scaled 
conjugate gradient method \parencite{Mbalawataa}.

By marginalizing the joint distribution of equation~\eqref{eq:joint_per_kalmanstep}
we get 
\begin{align}
	\Pdf{\y_k}{\y_{1:k-1},\Th}&=\N[\yk]{\v{H}\m_{k|k-1},\v{S}_k }.
\end{align}
Applying equation~\eqref{eq:lh_factorization} and taking the logarithm then gives
\begin{align}
	\lLH&=-\frac{1}{2}\sum_{k=1}^N\log\abbs{\v{S}_k}
	-\frac{1}{2}\sum_{k=1}^N\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)^T\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)+C,
	\label{eq:logLH}
\end{align}
where $C$ is a constant that doesn't depend on $\gv{\theta}$ and thus can
be ignored in the maximization.
Employing an efficient numerical optimization method generally
requires that the gradient of the objective function is available. 
There are at least two seemingly quite different methods for computing
the gradient of $\lLH$. The first one proceeds straightforwardly by taking the
partial derivatives of $\lLH$. As will soon be demonstrated, this leads
to some additional recursive formulas which allow computing
the gradient in parallel with the Kalman filter. The second method needs
the smoothing distributions with the cross-timestep covariances of equation~\eqref{eq:rts_cross_timestep_covariance}
and it can be easily computed with the expectation maximization machinery
that will be introduced later. These two methods can be proved to compute
the exact same quantity. At this point we will focus on the first one. 

In order to calculate the gradient of $\lLH$, we can take the partial
derivatives of it w.r.t every parameter $\theta_i$ in $\gv{\theta}$:

\begin{align}
\begin{split}
	\dpd{\lLH}{\theta_i}
	=&-\frac{1}{2}\sum_{k=1}^N\mathrm{Tr}\left(\v{S}_{k}^{-1}\dpd{\v{S}_k}{\theta_i}\right)\\
	&+\sum_{k=1}^N\left(\v{H}_k\dpd{\v{m}_{k|k-1}}{\theta_i}\right)^T\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)\\
	&+\frac{1}{2}\sum_{k=1}^N\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)^T\v{S}_{k}^{-1}\left(\dpd{\v{S}_k}{\theta_i}\right)\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)\\
	\label{eq:dlogLH}
\end{split}
\end{align}
From the Kalman filter recursions \eqref{eq:Kalman_filter} we find out that 
\begin{align}
	\dpd{\v{S}_k}{\theta_i}&=\v{H}\dpd{\v{P}_{k|k-1}}{\theta_i}\v{H}+\dpd{\v{R}}{\theta_i}
\end{align}
so that we're left with the task of determining the partial derivatives for
$\v{m}_{k|k-1}$ and $\v{P}_{k|k-1}$:

\begin{align}
	\dpd{\v{m}_{k|k-1}}{\theta_i}&=\dpd{\v{A}}{\theta_i}\v{m}_{k-1|k-1}+\v{A}\dpd{\v{m}_{k-1|k-1}}{\theta_i} \label{eq:m_pred_pd}\\
	\begin{split}
	\dpd{\v{P}_{k|k-1}}{\theta_i}&=\dpd{\v{A}}{\theta_i}\v{P}_{k-1|k-1}\v{A}^T+\v{A}\dpd{\v{P}_{k-1|k-1}}{\theta_i}\v{A}^T\\
	&+\v{A}\v{P}_{k-1|k-1}\left(\dpd{\v{A}}{\theta_i}\right)^T+\dpd{\v{Q}}{\theta_i} \label{eq:P_pred_pd}
	\end{split}
\end{align}
as well as for $\v{m}_{k|k}$ and $\v{P}_{k|k}$:
\begin{align}
	\dpd{\v{K}_k}{\theta_i}&=\dpd{\v{P}_{k|k-1}}{\theta_i}\v{H}^T\v{S}_{k}^{-1}-\v{P}_{k|k-1}\v{H}^T\v{S}_{k}^{-1}\dpd{\v{S}_k}{\theta_i}\v{S}_{k}^{-1}
	\label{eq:K_pd}\\
	\dpd{\v{m}_{k|k}}{\theta_i}&=\dpd{\v{m}_{k|k-1}}{\theta_i}+\dpd{\v{K}_k}{\theta_i}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)-\v{K}_k\v{H}\dpd{\v{m}_{k|k-1}}{\theta_i}
	\label{eq:m_pd}\\
	\dpd{\v{P}_{k|k}}{\theta_i}&=\dpd{\v{P}_{k|k-1}}{\theta_i}-\dpd{\v{K}_k}{\theta_i}\v{S}_{k}\v{K}_{k}^T-\v{K}_{k}\dpd{\v{S}_k}{\theta_i}\v{K}_{k}^T-\v{K}_{k}^T\v{S}_{k}\left(\dpd{\v{K}_k}{\theta_i}\right)^T
	\label{eq:P_pd}
	\end{align}
Equations \eqref{eq:m_pred_pd}, \eqref{eq:P_pred_pd}, \eqref{eq:K_pd}, \eqref{eq:m_pd} and \eqref{eq:P_pd} together specify
a recursive algorithm for computing \eqref{eq:dlogLH} that can be run alongside the Kalman filter recursions.
As noted in \textcite{Cappe2005}, these equations are sometimes known as the \emph{sensitivity equations}
and they are derived at least in \textcite{Gupta1974} and \textcite{Mbalawataa}.
\todo{Elaborate on nonlinear programming}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Expectation maximization (EM)}%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The expectation maximization (EM) algorithm \parencite{Dempster1977} is a general
method for finding ML and MAP estimates in probabilistic models with missing data or
latent variables \parencite{Bishop2006,barber2012bayesian}. As will be seen, instead of maximizing
\eqref{eq:logLH} directly, EM alternates between forming a variational lower bound and maximizing it.
We shall use $\E{\cdot}{q}\equiv\defint{}{}{\cdot \Pdf[q]{z}}{z}$ to denote the expectation
over some arbitrary distribution $\Pdf[q]{z}$.
Let us introduce a ``variational'' 
distribution $\tPX$ over the states, parameterized with $\Th'$ (not necessarily related to $\Th$).
Noting now that $\Pdf{\X}{\Y,\Th}=\cLH/\LH$ and that $\lLH\equiv\log\LH$ is independent of $\X$ we can then perform the
following decomposition on the log likelihood:
\begin{align}
	\lLH &= \log\cLH - \log\post \nonumber\\
	&= \E{\log\cLH}_{\tP} - \E{\log\post}_{\tP} \nonumber\\
	&= \underbrace{\E{\log\cLH-\log\tPX}_{\tP}}_{\LB[1pt]{\tP,\Th}}
	\underbrace{-\E{\log\post-\log\tPX}_{\tP}}_{\KL{\tP}{\post[1pt]}}
	\label{eq:lLH_decomp}
\end{align}
The important step here is taking the expectation over $\tPX$, since the \emph{complete-data log-likelihood}
$\log\cLH$ cannot be evaluated as $\X$ is unobserved.
Since $\KL{\tP}{\post}$, the \emph{Kullback-Leibler divergence} between $\tPX$ and $\post$, is always nonnegative,
we see that 
\begin{align}
	\lLH&\geq \LB{\tP,\Th} \label{eq:em_lh_lb}
\end{align}
with equality when 
\begin{align}
	\tPX &= \post, \label{eq:em_maxq}
\end{align}
i.e. the posterior distribution of the states with equal parameter value $\Th'=\Th$. Considered as a functional
of only $\tP$, clearly $\LB{\tP,\Th}$ is maximized
and $\KL{\tP}{\post}$ vanishes by \eqref{eq:em_maxq}. 
The nonnegativeness of the Kullback-Leibler divergence can be proved by
noting that $-\log$ is a convex function and so \emph{Jensen's inequality}
can be applied \parencite{Bishop2006}.

Let us take a closer at the the first term in \eqref{eq:lLH_decomp} with $\tPX=\Pdf{\X,\Y}{\Th'}$:
\begin{align}
	\LB{\Pdf{\X,\Y}{\Th'},\Th}&=
	\underbrace{\E{\log\cLH}_{\Pdf[p][1pt]{\X,\Y}{\Th'}}}_{\F[1pt]{\Lb}{\Th',\Th}} 
	-\E{\log\Pdf{\X,\Y}{\Th'}}_{\Pdf[p][1pt]{\X,\Y}{\Th'}}.
	\label{eq:completedata_loglikelihood}
\end{align}
Clearly the latter term (the differential entropy of $\Pdf{\X,\Y}{\Th'}$) is constant 
with respect to $\Th$, so that maximizing $\mathcal{L}$ with respect to $\Th$ amounts to
maximizing $\F{\Lb}{\Th',\Th}$, the \emph{expected complete-data log-likelihood},
with respect to $\Th$.


We are now ready the define the EM algorithm, which produces
a series of estimates $\{\Th_j\}$ to the parameter $\Th$
starting from an initial guess $\Th_0$. The two alternating
steps of the algorithm are:

\begin{description}
\addtolength{\leftskip}{1cm}
  \item[E-step]\hfill\\
  Given the current estimate $\Th_j$ of the parameters, compute	
  	\begin{align}
		\tP_{j+1} &= \argmax_{\tP}\LB{\tP,\Th_j}.
		\label{eq:EM_E}
	\end{align}
  As stated, the maximum is obtained with 
  $\tP_{j+1} = \Pdf{\X}{\Y,\Th_j}$, the posterior
  distribution of the states given the current parameter estimate. After the maximization
  we have
  	\begin{align}
  		\lLH[\Th_j]&=\LB{\Pdf{\X}{\Y,\Th_j},\Th_j} \label{eq:EM_E_lh}\\
  		 &= \F{\Lb}{\Th_j,\Th_j}+\mathtt{const}\nonumber 
	\end{align}
  \item[M-step]\hfill\\ 
  Set
    \begin{align}
		\Th_{j+1}&=\argmax_{\Th}\LB{\tP_{j+1},\Th} \label{eq:EM_M}\\
		&=\argmax_{\Th}\F{\Lb}{\Th_j,\Th}\nonumber.
	\end{align}
\end{description}
We are now in a position to formulate the so called \emph{fundamental inequality of EM} \parencite{Cappe2005}:
\begin{align}
	\lLH[\Th_{j+1}] - \lLH[\Th_j]\geq & \LB{\Pdf{\X}{\Y,\Th_j},\Th_{j+1}} - \LB{\Pdf{\X}{\Y,\Th_j},\Th_{j}} 
	\label{eq:fundamental_inequality}
\end{align}
which is just the combination of \eqref{eq:em_lh_lb} and \eqref{eq:EM_E_lh}. But it highlights
the fact that \emph{the likelihood is increased or unchanged with every new estimate} $\Th_{j+1}$.
Also following from \eqref{eq:fundamental_inequality} is the fact that if the iterations
stop at a certain point, i.e. $\Th_{l+1}=\Th_l$ at iteration $l$, then
$\LB{\Pdf{\X}{\Y,\Th_l},\Th}$ must be maximal at $\Th_l$
and so the gradients of the lower bound and of the likelihood must be zero. Thus
$\Th_l$ is a \emph{stationary point} of $\lLH$, i.e a local maximum or a saddle point.


\subsubsection{EM as a special case of variational Bayes}
\todo{Ensure that notation matches prev chapter}

\parencite{barber2012bayesian,jordan1998learning}
Variational Bayes (VB) is a fully Bayesian methodology where one seeks
for an approximation to the parameter posterior
\begin{align}
	\Pdf{\Th}{\Y}\propto \defint{\mathcal{X}}{}{\Pdf{\X,\Y}{\Th}}{\X}\Pdf{\Th}
	\label{tablelabel}
\end{align}
As mentioned earlier, finding this distribution is commonly intractable, so in VB
we assume a factorized form for the joint posterior of states and parameters
\begin{align}
	\Pdf{\X,\Th}{\Y}\approx \Pdf[q]{\X}\Pdf[q]{\Th}
	\label{eq:VB_factorization}
\end{align}
and the task is then to find the best approximation with respect
to the KL divergence between the true posterior and the approximation
\begin{align}
	\KL{\Pdf[q]{\X}\Pdf[q]{\Th}}{\Pdf{\X,\Th}{\Y}} &= \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} + \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} -
	\E{\Pdf{\X,\Th}{\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}}.
	\label{eq:KL_VB}
\end{align}
Using ${\Pdf{\X,\Th}{\Y}}=\Pdf{\X,\Th,\Y}/\Pdf{\Y}$ equation \eqref{eq:KL_VB} gives
\begin{align}
	\lLH &\geq \E{\Pdf{\X,\Th,\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}} - \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} -
	\E{\Pdf[q]{\X}}{\Pdf[q]{\X}}
	\label{eq:VB_bound}
\end{align}
and thus minimizing the KL divergence is equivalent to finding the tightest lower bound to
the log likelihood. Analogously to EM, minimizing the KL divergence is done iteratively
keeping $\Pdf[q]{\Th}$ fixed and minimizing w.r.t $\Pdf[q]{\X}$ in the ``E''-step
and vice versa in the ``M''-step:

\begin{description}
\addtolength{\leftskip}{1cm}
\item[E-step]
\begin{align}
	\Pdf[q^{\text{new}}]{\X}=\argmax_{\Pdf[q]{\X}}\left(\KL{\Pdf[q]{\X}\Pdf[q^{\text{old}}]{\Th}}{\Pdf{\X,\Th}{\Y}}\right)
	\label{eq:VB_E}
\end{align}
\item[M-step]
\begin{align}
	\Pdf[q^{\text{new}}]{\Th}=\argmax_{\Pdf[q]{\Th}}\left(\KL{\Pdf[q^{\text{new}}]{\X}\Pdf[q]{\Th}}{\Pdf{\X,\Th}{\Y}}\right)
	\label{eq:VB_M}
\end{align}
\end{description}


Let us then suppose that we only wish to find the MAP point estimate $\Th^*$. This can be accomplished
by assuming a delta function form $\Pdf[q]{\Th}=\delta\left(\Th,\Th^*\right)$ for the parameter factor in the
joint distribution of states and parameters \eqref{eq:VB_factorization}.
With this assumption equation \eqref{eq:VB_bound} becomes
\begin{align}
	\Pdf{\Y}{\Th^*} &\geq \E{\Pdf{\X,\Th^*,\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}} - \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} + \mathtt{const}
	\label{eq:VB_MAP_boundl}
\end{align}
and the ``M''-step \eqref{eq:VB_M} can then be written as
\begin{align}
	\Th^* &= \text{argmax}_{\Th}\left(\E{\log\cLH}{\Pdf[q]{\X}}+\log\Pdf{\Th}\right).
	\label{tablelabel}
\end{align}
If the point estimate is plugged in the ``E''-step equation \eqref{eq:VB_E} we have
\begin{align}
	\Pdf[q^{\text{new}}]{\X}\propto \Pdf{\X,\Y}{\Th^*} \propto \Pdf{\X}{\Y,\Th^*} 
	\label{tablelabel}
\end{align}

\subsubsection{Partial E and M steps}


\subsubsection{Gradient computation}
Another property of the lower bound worth stating formally is the following: assume that
the likelihood and \eqref{eq:KL} are continuously differentiable, then
\begin{align}
		\left.\dpd{\lLH}{\theta_i}\right|_{\Th=\widehat\Th_j}&=\left.\dpd{\LB{\Pdf{\X}{\Y,\widehat\Th}_j,\Th}}{\theta_i}\right|_{\Th=\widehat\Th_j} \label{eq:EM_gradients}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Applying EM}%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Let us then look at how to apply EM to a SSM of the form \eqref{eq:ssm_general}. First of
all, from the factorization in \eqref{eq:complete_data_likelihood}, the complete-data log-likelihood becomes
\begin{align*}
\begin{split}
	\cLH =&-\frac{1}{2}\left(\x_0-\gv{\mu}_0\right)^T\gv{\Sigma}_0^{-1}\left(\x-\gv{\mu}_0\right)-\frac{1}{2}\log\abbs{\gv{\Sigma}_0}\\
	&-\frac{1}{2}\sum_{k=1}^T\left(\x_k-\v{f}(\x_{k-1})\right)^T\v{Q}^{-1}\left(\x_k-\v{f}(\x_{k-1})\right)-\frac{T}{2}\log\abbs{\v{Q}}\\
	&-\frac{1}{2}\sum_{k=1}^T\left(\y_k-\v{h}(\x_{k})\right)^T\v{R}^{-1}\left(\y_k-\v{h}(\x_{k})\right)-\frac{T}{2}\log\abbs{\v{R}}\\
	&+\mathtt{const}
\end{split}
\end{align*}
Taking the expectation w.r.t. $\Pdf{\X}{\Y,\Th'}$ (assumed implicitly in the notation), applying the identity $\x^T\X\x=\Tr{\x^T\X\x}=\Tr{\X\x\x^T}$
and using $\f_{k-1}\equiv \f(\x_{k-1})$ and $\h_k\equiv \h(\x_k)$
\begin{align}
\begin{split}
	\F{\Lb}{\Th',\Th} =&-\frac{1}{2}\Tr{\gv{\Sigma}_0^{-1}\E{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^T}}-\frac{1}{2}\log\abbs{\gv{\Sigma}_0}\\
	&-\frac{1}{2}\sum_{k=1}^T\Tr{\v{Q}^{-1}\E{\left(\x_k-\f_{k-1}\right)\left(\x_k-\f_{k-1}\right)^T}}-\frac{T}{2}\log\abbs{\v{Q}}\\
	&-\frac{1}{2}\sum_{k=1}^T\Tr{\v{R}^{-1}\E{\left(\y_k-\h_{k}\right)\left(\y_k-\h_{k}\right)^T}}-\frac{T}{2}\log\abbs{\v{R}}\\
	&+\mathtt{const}.
\end{split}
\label{eq:eclLH}
\end{align} 
Let us denote the three expectations in equation~\eqref{eq:eclLH} with
\begin{align}
	\v{I}_1 &= \E{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^T}\\ 
	&=\defint{\mathcal{X}\times T}{}{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^T\Pdf{\X}{\Y,\Th'}}{\X}\nonumber\\
	&= 	\defint{\mathcal{X}}{}{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^T\Pdf{\x_0}{\Y,\Th'}}{\x_0}\\
	\v{I}_{2,k} &= \defint{\mathcal{X}\times 2}{}{\left(\x_k-\f_{k-1}\right)\left(\x_k-\f_{k-1}\right)^T\Pdf{\bm{\xk&\xkk}^T}{\Y,\Th'}}{\bm{\xk&\xkk}^T}\\
	\v{I}_{3,k} &= \defint{\mathcal{X}}{}{\left(\y_k-\h_{k}\right)\left(\y_k-\h_{k}\right)^T\Pdf{\xk}{\Y,\Th'}}{\xk}
\end{align}
It is clear then that in the E-step one needs to compute the $T+1$ smoothing
distributions, including the $T$ cross-timestep distributions, since these
will be needed in the expectations.
By applying the identity
\begin{align}
	\var{\x}&=\E{\x\x^T}-\E{\x}\E{\x}^T,
\end{align} 
we can already write the first expectation as
\begin{align}
	\v{I}_1&= \v{P}_{0|T}+(\v{m}_{0|T}-\gv{\mu}_0)(\v{m}_{0|T}-\gv{\mu}_0)^T.
	\label{tablelabel}
\end{align}
Thus in the following more specific cases, we will only
consider the two remaining expectations and the M-step.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EM in linear-Gaussian SSM:s}%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\parencite{shumway1982approach,Ghahramani1996}
Let us substitute $\v{A}\xkk$ for $\f_{k-1}$ and $\v{H}\xk$ for $\h_{k}$.
Let us also denote by
\begin{align}
	\Pdf{\xk, \xkk}{\v{Y},\Th}&=
	\N[
	\begin{bmatrix}
		\xk\\\xkk
	\end{bmatrix}
	]{\m_{k,k-1|T},\P_{k,k-1|T}},
\end{align}
the joint smoothing distribution of $\xk$ and $\xkk$.
Then by applying the manipulation
\begin{align}
\begin{split}
&\E{\left(\v{x}_k-\v{A}\v{x}_{k-1}\right)\left(\v{x}_k-\v{A}\v{x}_{k-1}\right)^T}\\
=&\bm{\v{I} & -\v{A}}	
\E{
\begin{bmatrix}
	\xk\\\xkk
\end{bmatrix}
\begin{bmatrix}
	\xk^T & \xkk^T	
\end{bmatrix}
}
\bm{\v{I}\\-\v{A}^T}	
\end{split}
\end{align}
we get
\begin{align}
	\v{I}_{2,k}&=
\bm{\v{I} & -\v{A}}	
\left(\P_{k,k-1|T}+\m_{k,k-1|T}\m_{k,k-1|T}^T\right)
\bm{\v{I}\\-\v{A}^T}\\
	\v{I}_{3,k}&=\v{H}\P_{k|T}\v{H}^T+\left(\yk-\v{H}\m_{k|T}\right)
	\left(\yk-\v{H}\m_{k|T}\right)^T
\end{align}

All in all, the E-step of the 
EM algorithm in linear-Gaussian SSM:s corresponds to computing
the matrices in \eqref{eq:sum_expectations} with the help of the Kalman filter and
the RTS smoother. In \parencite{Elliott1999} a new kind of filter is presented that
can compute \eqref{eq:sum_expectations} with only forward recursions. 


%\subsubsubsection{M-step with structured matrices}
\cite{Wills2011}
When we want to maximize $\Lb(\Th,\hat{\Th})$ w.r.t some other
parameters than the ones in $\Th_M$, the situation becomes more complicated.
In the general case, no analytical formulas can be found. We therefore seek
to maximize $\Lb(\Th,\hat{\Th})$ numerically, analogously to how $L(\gv{\theta})$
was maximized in section~\ref{sec:grad}.

Fortunately calculating the gradient of $\Lb(\Th,\hat{\Th})$ is straightforward:

\begin{align}
\begin{split}
	-2\dpd{\Lb(\Th,\hat{\Th}))}{\theta_i}
	=&\Tr{-\v{Q}^{-1}\dpd{\v{Q}}{\theta_i}\v{Q}^{-1}
	\left(\v{B}_1-\v{A}\v{B}_2^T-\v{B}_2\v{A}^T+\v{A}\v{B}_3\v{A}^T\right)}\\
	&+\Tr{\v{Q}^{-1}\left(-\dpd{\v{A}}{\theta_i}\v{B}_2^T-\v{B}_2\dpd{\v{A}}{\theta_i}^T+\dpd{\v{A}}{\theta_i}\v{B}_3\v{A}^T+\v{A}\v{B}_3\dpd{\v{A}}{\theta_i}^T\right)}\\
	&+\Tr{-\v{R}^{-1}\dpd{\v{R}}{\theta_i}\v{R}^{-1}
	\left(\v{B}_4-\v{H}\v{B}_5^T-\v{B}_5\v{H}^T+\v{H}\v{B}_1\v{H}^T\right)}\\
	&+N\Tr{\v{Q}^{-1}\dpd{\v{Q}}{\theta_i}}
	+N\Tr{\v{R}^{-1}\dpd{\v{R}}{\theta_i}}\\
	\label{eq:dLB}
\end{split}
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EM in linear-in-the-parameters SSM:s}%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:litp}

Suppose the function $\v{f}$ is linear in the parameters and the dimension of the state $\x$ is $d$.
Then in the most general case $\v{f}(\x):\R^d\to\R^d$ is a linear
combination of vector valued functions $\gv{\rho}_k(\x):\R^d\to\R^{d_k}$  
and the parameters are matrices $\gv{\Phi}_k\in\R^{d\times d_k}$. More specifically,
we have

\begin{align}
\begin{split}
	\v{f}(\x)&=\gv{\Phi}\gv{\rho}_1(\x)+\dots+\gv{\Phi}_m\gv{\rho}_m(\x)\\
	&=
	\begin{bmatrix}
		\gv{\Phi}_1 & \dots & \gv{\Phi}_m
	\end{bmatrix}
	\begin{bmatrix}
		\gv{\rho}_1(\x)\\
		\vdots\\ 
		\gv{\rho}_m(\x)
	\end{bmatrix}\\
	&=\v{A}\v{g}(\x),
\end{split}
\end{align}
where $\v{A}\in\R^{d\times\sum_{k=1}^m d_k}$ and $\v{g}(\x):\R^d\to \R^{\sum_{k=1}^m d_k}$ . 
For example, in case of the function
\begin{align}
	f(x,t)&=ax+b\frac{x}{1+x^2}+c\cos(1.2t)
\end{align}
we would have
\begin{align}
	f(x,t)&=
	\begin{bmatrix}
		a & b & c
	\end{bmatrix}
	\begin{bmatrix}
		x\\
		\frac{x}{1+x^2}\\ 
		\cos(1.2t)
	\end{bmatrix}
\end{align}
Suppose now, that the matrix $A$ depends on parameters $\v{s}$.
Let us then deduce the maximization equation for the parameter $s_i$:
\begin{align}
	-2\dpd{\Lb(\Th,\hat{\Th}_j)}{s_i}&=
	\sum_{k=1}^N\E{\dpd{}{s_i}\left(\x_k-\v{A}\v{g}(\x_{k-1})\right)^T\v{Q}^{-1}\left(\x_k-\v{A}\v{g}(\x_{k-1})\right)}{\hat{\Th}_j}\label{eq:s_grad}
\end{align}
and
\begin{align}
	&\dpd{}{s_i}\left(\x_k-\v{A}\v{g}(\x_{k-1})\right)^T\v{Q}^{-1}\left(\x_k-\v{A}\v{g}(\x_{k-1})\right)\\
	=&-2\v{g}(\x_{k-1})^T\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\left(\x_k-\v{A}\v{g}(\x_{k-1})\right)\\
	=&-2\Tr{\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\x_k\v{g}(\x_{k-1})^T}-2\Tr{\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\v{A}\v{g}(\x_{k-1})\v{g}(\x_{k-1})^T}\\
	=&-2\Tr{\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\left(\x_k\v{g}(\x_{k-1})^T-\v{A}\v{g}(\x_{k-1})\v{g}(\x_{k-1})^T\right)}\label{eq:a_grad}.
\end{align}
Combining equations \eqref{eq:a_grad} and \eqref{eq:s_grad} then gives
\begin{multline}
	\dpd{\Lb(\Th,\hat{\Th}_j)}{s_i}=\\
	-2\Tr{\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\left(\sum_{k=1}^N\E{\x_k\v{g}(\x_{k-1})^T}-\v{A}\sum_{k=1}^N\E{\v{g}(\x_{k-1})\v{g}(\x_{k-1})^T}\right)}
	\label{eq:pdcLH_nonlinear}
\end{multline}
If we then set \eqref{eq:pdcLH_nonlinear} to zero, we have
\begin{align}
	\v{A}_{j+1}=\left(\sum_{k=1}^N\E{\x_k\v{g}(\x_{k-1})^T}\right)\left(\sum_{k=1}^N\E{\v{g}(\x_{k-1})\v{g}(\x_{k-1})^T}\right)^{-1}
\end{align}
which is similar to \eqref{eq:A_new}

Suppose the function $\v{h}$ is linear in the parameters and the dimension of
the state $\x$ is $d^x$ and of the measurements $d^y$.
Then in the most general case $\v{h}(\x):\R^{d^x}\to\R^{d^y}$ is a linear
combination of vector valued functions $\gv{\pi}_k(\x):\R^{d^x}\to\R^{d_k}$  
and the parameters are matrices $\gv{\Upsilon}_k\in\R^{d^x\times d_k}$. More
specifically, we have

\begin{align}
\begin{split}
	\v{h}(\x)&=\gv{\Upsilon}\gv{\pi}_1(\x)+\dots+\gv{\Upsilon}_m\gv{\pi}_m(\x)\\
	&=
	\begin{bmatrix}
		\gv{\Upsilon}_1 & \dots & \gv{\Upsilon}_m
	\end{bmatrix}
	\begin{bmatrix}
		\gv{\pi}_1(\x)\\
		\vdots\\ 
		\gv{\pi}_m(\x)
	\end{bmatrix}\\
	&=\v{H}\v{l}(\x),
\end{split}
\end{align}
  
Suppose now, that the matrix $\v{H}$ depends on parameters $\v{t}$.
Let us then deduce the maximization equation for the parameter $t_i$:
\begin{align}
	-2\dpd{\Lb(\Th,\hat{\Th}_j)}{t_i}&=
	\frac{1}{2}\Tr{\v{R}^{-1}\sum_{k=1}^N\E{\dpd{}{t_i}\left(\y_k-\v{H}\v{l}(\x_k)\right)\left(\y_k-\v{H}\v{l}(\x_k)\right)^T}{\hat{\Th}_j}}
\label{eq:s_grad}
\end{align}
and

If we then set \eqref{eq:pdcLH_nonlinear} to zero, we have
\begin{align}
	\v{H}_{j+1}=\left(\sum_{k=1}^N\y_k\E{\v{l}(\x_{k})^T}\right)\left(\sum_{k=1}^N\E{\v{l}(\x_{k})\v{l}(\x_{k})^T}\right)^{-1}
\end{align}
which is similar to \eqref{eq:H_new}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EM in nonlinear-Gaussian SSM:s}%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As explained in section \ref{sec:nonlinear_state}, in then nonlinear case
the filtering and smoothing distributions cannot be computed exactly.
Thus the E-step solution is also only approximate and the convergence
guarantees of EM won't apply anymore. The proposed methods can nevertheless
provide good results most of the time. In the fortunate case that the
model is linear-in-the-parameters the M-step can be solved in closed form.
This situation will be covered later in section~\ref{sec:litp}.

\begin{align}
	\dpd{\Lb(\Th,\hat{\Th}))}{\theta_i}&=\dpd{I_1}{\theta_i}+\dpd{I_3}{\theta_i}+\dpd{I_3}{\theta_i}
	\label{eq:dLB_nonlinear_1}
\end{align}
and
\begin{align}
\begin{split}
	-2\dpd{I_2}{\theta_i}
	=&-\Tr{\v{Q}^{-1}\dpd{\v{Q}}{\theta_i}\v{Q}^{-1}
	\sum_{k=1}^N\E{\left(\x_k-\v{f}(\x_{k-1})\right)\left(\x_k-\v{f}(\x_{k-1})\right)^T}{\hat{\Th}_j}}\\
	&-\Tr{\v{Q}^{-1}\sum_{k=1}^N\E{\dpd{\v{f}(\x_{k-1})}{\theta_i}\left(\x_k-\v{f}(\x_{k-1})\right)^T+\left(\x_k-\v{f}(\x_{k-1})\right)\dpd{\v{f}^T(\x_{k-1})}{\theta_i}}{\hat{\Th}_j}}\\
	&+N\Tr{\v{Q}^{-1}\dpd{\v{Q}}{\theta_i}}\\
	\dpd{I_3}{\theta_i}=&
	-\Tr{\v{R}^{-1}\dpd{\v{R}}{\theta_i}\v{R}^{-1}
	\sum_{k=1}^N\E{\left(\y_k-\v{h}(\x_{k})\right)\left(\y_k-\v{h}(\x_{k})\right)^T}{\hat{\Th}_j}}\\
	&-\Tr{\v{R}^{-1}\sum_{k=1}^N\E{\dpd{\v{h}(\x_{k})}{\theta_i}\left(\y_k-\v{h}(\x_{k})\right)^T+\left(\y_k-\v{h}(\x_{k})\right)\dpd{\v{h}^T(\x_{k})}{\theta_i}}{\hat{\Th}_j}}\\
	&+N\Tr{\v{R}^{-1}\dpd{\v{R}}{\theta_i}}\\
	\label{eq:dLB_nonlinear}
\end{split}
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%\subsubsection{M-step maximization equations}%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let us separate the set of all parameters $\Th$ into
subsets

\begin{align}
	\Th&=\left\{\gv{\psi},\gv{\omega},\v{Q},\v{R}\right\},
\end{align}
where $\gv{\psi}$ are the parameters of $f$ and $\gv{\omega}$ are
the parameters of $h$ (these sets can intersect). Let us first consider
the maximization with respect to $\v{Q}$. We have

\begin{multline}
	-2\dpd{\Lb(\Th,\hat{\Th}_j)}{\v{Q^{-1}}}=
	\sum_{k=1}^N\dpd{}{\v{Q^{-1}}}\Tr{\v{Q}^{-1}\E{\left(\x_k-\v{f}(\x_{k-1})\right)\left(\x_k-\v{f}(\x_{k-1})\right)^T}{\hat{\Th}_j}}\\+N\dpd{}{\v{Q^{-1}}}\log\abbs{\v{Q}}
	\label{eq:clLH_pdQ}
\end{multline}
Using formula 92 in \cite{Petersen2008} for the first derivative and formula 51 for the second, we get
\begin{align}
	-2\dpd{\Lb(\Th,\hat{\Th}_j)}{\v{Q^{-1}}}&=
	\sum_{k=1}^N\E{\left(\x_k-\v{f}(\x_{k-1})\right)\left(\x_k-\v{f}(\x_{k-1})\right)^T}{\hat{\Th}_j}-N\v{Q}
	\label{eq:clLH_pdQ2}
\end{align}
and setting this to zero we get the update equation for the next estimate of $\v{Q}$
\begin{align}
	\v{Q}_{j+1}&=\frac{1}{N}\sum_{k=1}^N\E{\left(\x_k-\v{f}(\x_{k-1})\right)\left(\x_k-\v{f}(\x_{k-1})\right)^T}{\hat{\Th}_j}
\end{align}
The derivation of the update equation for the next estimate of $\v{R}$ is exactly analogous, giving
\begin{align}
	%-2\dpd{\Lb(\Th,\hat{\Th}_j)}{\v{R^{-1}}}&=\sum_{k=1}^N\E{\left(\y_k-\v{g}(\x_{k})\right)^T\left(\y_k-\v{g}(\x_{k})\right)}{\hat{\Th}_j}-N\v{R}\\
	\v{R}_{j+1}&=\frac{1}{N}\sum_{k=1}^N\E{\left(\y_k-\v{h}(\x_{k})\right)\left(\y_k-\v{h}(\x_{k})\right)^T}{\hat{\Th}_j}
\end{align} 


\subsection{Comparisons}
\subsubsection{Convergence}
\subsubsection{Computational complexity}
\parencite{Harvey1990,Watson1983,Cappe2005,Saatci2011,Olsson2007,Salakhutdinov2003a}




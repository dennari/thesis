%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Maximum likelihood and maximum a posteriori estimation}%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the Bayesian sense the complete answer to the parameter estimation
problem is the marginal posterior probability of the parameters
given the measurements, which is given by Bayes' rule as
\begin{align}
	\Pdf{\gv{\theta}}{\v{Y}}&=\frac{\Pdf{\v{Y}}{\gv{\theta}}\Pdf{\gv{\theta}}}{\Pdf{\v{Y}}}.
	\label{eq:param_post}
\end{align}

Computing the posterior distribution of the parameters is usually intractable \todo{why? examples?}. A much
easier problem is finding a suitable \emph{point estimate} $\hat{\gv{\theta}}$.
This effectively means that we don't need to worry about the normalizing
term $\Pdf{\v{Y}}$, since it's constant with respect to the parameters. 
A point estimate that maximizes the posterior distribution
is called a \emph{maximum a posteriori} (MAP) estimate. 
Since the logarithm is a strictly monotonic function, maximizing a function
is the same as maximizing its logarithm. Thus the MAP estimate $\Th^*$ is given by 
\begin{align}
	\Th_{\text{MAP}} &= \text{argmax}_{\Th}\left[\underbrace{\log \Pdf{\v{Y}}{\gv{\theta}}}_{\Pdf[\ell][2pt]{\Th}} + \log\Pdf{\gv{\theta}}\right]
	\label{eq:MAP}
\end{align}

In the case of a flat (constant and thus improper)
prior distribution, $\Pdf{\Th}$, the MAP estimate converges to the
\emph{maximum likelihood} (ML) estimate
\begin{align}
	\Th_{\text{ML}} &= \text{argmax}_{\Th}\left[\lLH\right]
	\label{eq:ML}
\end{align}
Going further we we will only be concerned with finding the ML estimate, but it should
be remembered that both of the methods we consider can be extended
to the estimation of the MAP estimate in a straightforward fashion.
\todo{Explain MAP in both cases}

Among different point estimates, the maximum likelihood estimator has good statistical properties.
Let us denote the true parameter value, the value that the data was generated with, with $\Th_\star$ and 
let $T$ denote the amount of observations.
Then we can state the following asymptotic properties for the ML estimate $\Th_{\text{ML}}$:

\begin{description}
\addtolength{\leftskip}{1cm}
\item[Strong consistency]
\begin{align}
	%\forall \Th \in \Theta\quad \frac{1}{n}\Pdf[\ell_n]{\Th} \xrightarrow{\mathrm{a.s.}} \lLH, \mathrm{when} n\to\infty
	\Th_{\text{ML}} \xrightarrow{\mathrm{a.s.}} \Th_\star,\quad \mathrm{when}\;T\to\infty
\end{align}
%where $\Pdf[\ell_n]{\Th}$ is the log-likelihood given $n$ observations and $\lLH$ is a continuous deterministic
%function with a unique global maximum at $\Th_\star$.
\item[Asymptotic normality]

\end{description}


\subsubsection{Identifiability}

Since the parameters are determined by maximizing 

\parencite{Haykin2001,Cappe2005}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient based nonlinear optimization}\label{sec:grad}%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This is the classical way of solving the parameter estimation problem. It consists
of computing the gradient of the log-likelihood function and then using some
non-linear optimization method to find a \emph{local} maximum to it.\parencite{Mbalawataa}. 
An efficient non-linear optimization algorithm is the scaled 
conjugate gradient method \parencite{Mbalawataa}.

To derive the expression for the log-likelihood function in our case,
let us first see what the Kalman filter calculates. Firstly,
the recursions are as follows \parencite{Mbalawataa}:
\begin{subequations}
\begin{align}
	\shortintertext{prediction:}
	\v{m}_{k|k-1}&=\v{A}\v{m}_{k-1|k-1}\\
	\v{P}_{k|k-1}&=\v{A}\v{P}_{k-1|k-1}\v{A}^T+\v{Q}
	\shortintertext{update:}
	\v{v}_k&=\v{y}_k-\v{H}\v{m}_{k|k-1}\\
	\v{S}_k&=\v{H}\v{P}_{k|k-1}\v{H}^T+\v{R}\\
	\v{K}_k&=\v{P}_{k|k-1}\v{H}^T\v{S}_{k}^{-1}\\
	\v{m}_{k|k}&=\v{m}_{k|k-1}+\v{K}_k\v{v}_k\\
	\v{P}_{k|k}&=\v{P}_{k|k-1}-\v{K}_k\v{S}_k\v{K}_{k}^T
\end{align}
\end{subequations}
This includes the sufficient statistics for the $T$
joint distributions 
\begin{align}
\begin{split}
	\Pdf{\v{x}_k,\v{y}_k}{\v{y}_1,\dots,\v{y}_{k-1},\gv{\theta}}
	&=N\left(
	\begin{bmatrix}
		\v{x}_k\\\v{y}_{k}
	\end{bmatrix}\left\vert
	\begin{bmatrix}
		\v{m}_{k|k-1}\\
		\v{H}\v{m}_{k|k-1}
	\end{bmatrix}
	\right.,
	\begin{bmatrix}
		\v{P}_{k|k-1} & \v{P}_{k|k-1}\v{H}^T\\
		\v{H}\v{P}_{k|k-1}^T & \v{S}_k  
	\end{bmatrix}
	\right)\\
	\Rightarrow \Pdf{\v{y}_k}{\v{y}_1,\dots,\v{y}_{k-1},\gv{\theta}}
	&=N\left(\cond{\v{y}_k}{\v{H}\v{m}_{k|k-1},\v{S}_k }\right)
\end{split}
	\label{eq:joint_per_kalmanstep}
\end{align}
To see how this enables us to calculate the likelihood, one 
only needs to note that (it has been assumed that the observations are
independent given the states)
\begin{align}
	\Pdf{\v{Y}}{\gv{\theta}}&=\Pdf{\v{y}_1}{\gv{\theta}}\prod_{k=2}^N \Pdf{\v{y}_k}{\v{y}_1,\dots,\v{y}_{k-1},\gv{\theta}}
	\label{eq:marginal_lh_factorization}
\end{align}
Armed with this knowledge, we can write the following expression for the log-likelihood function in this linear-Gaussian
case:
\begin{align}
	-2L(\gv{\theta})&=\sum_{k=1}^N\log\abbs{\v{S}_k}
	+\sum_{k=1}^N\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)^T\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)+C,
	\label{eq:logLH}
\end{align}
where $C$ is a constant that doesn't depend on $\gv{\theta}$.
Employing an efficient numerical optimization method generally
requires that the gradient of the objective function, that is $L(\gv{\theta})$ in this case,
is available. In order to calculate the gradient of $L(\gv{\theta})$, we need to formally derivate
it w.r.t every parameter $\theta_i$ in $\gv{\theta}$:

\begin{align}
\begin{split}
	\dpd{L(\gv{\theta})}{\theta_i}
	&=-\frac{1}{2}\sum_{k=1}^N\mathrm{Tr}\left(\v{S}_{k}^{-1}\dpd{\v{S}_k}{\theta_i}\right)\\
	&+\sum_{k=1}^N\left(\v{H}_k\dpd{\v{m}_{k|k-1}}{\theta_i}\right)^T\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)\\
	&+\frac{1}{2}\sum_{k=1}^N\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)^T\v{S}_{k}^{-1}\left(\dpd{\v{S}_k}{\theta_i}\right)\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)\\
	\label{eq:dlogLH}
\end{split}
\end{align}
From the Kalman filter recursions we find out that 
\begin{align}
	\dpd{\v{S}_k}{\theta_i}&=\v{H}\dpd{\v{P}_{k|k-1}}{\theta_i}\v{H}+\dpd{\v{R}}{\theta_i}
\end{align}
so that we're left with the task of determining the partial derivatives for
$\v{m}_{k|k-1}$ and $\v{P}_{k|k-1}$:

\begin{align}
	\dpd{\v{m}_{k|k-1}}{\theta_i}&=\dpd{\v{A}}{\theta_i}\v{m}_{k-1|k-1}+\v{A}\dpd{\v{m}_{k-1|k-1}}{\theta_i} \label{eq:m_pred_pd}\\
	\begin{split}
	\dpd{\v{P}_{k|k-1}}{\theta_i}&=\dpd{\v{A}}{\theta_i}\v{P}_{k-1|k-1}\v{A}^T+\v{A}\dpd{\v{P}_{k-1|k-1}}{\theta_i}\v{A}^T\\
	&+\v{A}\v{P}_{k-1|k-1}\left(\dpd{\v{A}}{\theta_i}\right)^T+\dpd{\v{Q}}{\theta_i} \label{eq:P_pred_pd}
	\end{split}
\end{align}
as well as for $\v{m}_{k|k}$ and $\v{P}_{k|k}$:
\begin{align}
	\dpd{\v{K}_k}{\theta_i}&=\dpd{\v{P}_{k|k-1}}{\theta_i}\v{H}^T\v{S}_{k}^{-1}-\v{P}_{k|k-1}\v{H}^T\v{S}_{k}^{-1}\dpd{\v{S}_k}{\theta_i}\v{S}_{k}^{-1}
	\label{eq:K_pd}\\
	\dpd{\v{m}_{k|k}}{\theta_i}&=\dpd{\v{m}_{k|k-1}}{\theta_i}+\dpd{\v{K}_k}{\theta_i}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)-\v{K}_k\v{H}\dpd{\v{m}_{k|k-1}}{\theta_i}
	\label{eq:m_pd}\\
	\dpd{\v{P}_{k|k}}{\theta_i}&=\dpd{\v{P}_{k|k-1}}{\theta_i}-\dpd{\v{K}_k}{\theta_i}\v{S}_{k}\v{K}_{k}^T-\v{K}_{k}\dpd{\v{S}_k}{\theta_i}\v{K}_{k}^T-\v{K}_{k}^T\v{S}_{k}\left(\dpd{\v{K}_k}{\theta_i}\right)^T
	\label{eq:P_pd}
	\end{align}
Equations \eqref{eq:m_pred_pd}, \eqref{eq:P_pred_pd}, \eqref{eq:K_pd}, \eqref{eq:m_pd} and \eqref{eq:P_pd} together specify
a recursive algorithm for computing \eqref{eq:dlogLH} that can be run alongside the Kalman filter recursions.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Expectation maximization (EM)}%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The expectation maximization (EM) algorithm \parencite{Dempster1977} is a general
method for finding ML and MAP estimates in probabilistic models with missing data or
latent variables \parencite{Bishop2006,barber2012bayesian}. As will be seen, instead of maximizing
\eqref{eq:logLH} directly, EM alternates between forming a variational lower bound and maximizing it.
We shall use $\E{\cdot}{q}\equiv\defint{}{}{\cdot \Pdf[q]{z}}{z}$ to denote the expectation
over some arbitrary distribution $\Pdf[q]{z}$.
Let us introduce a ``variational'' (it will be a parameter in an optimization problem \parencite{barber2012bayesian}) 
distribution $\tPX$ over the states. 
Noting now that $\Pdf{\X}{\Y,\Th}=\cLH/\LH$ and that $\lLH\equiv\log\LH$ is independent of $\X$ we can then perform the
following decomposition on the log likelihood:
\begin{align}
	\lLH &= \lcLH - \log\post \nonumber\\
	&= \E{\lcLH}{\tP} - \E{\log\post}{\tP} \nonumber\\
	&= \underbrace{\E{\lcLH-\log\tPX}{\tP}}_{\LB\left(\tP,\Th\right)}
	\underbrace{-\E{\log\post-\log\tPX}{\tP}}_{\KL{\tP}{\post}}
	\label{eq:lLH_decomp}
\end{align}
Since $\KL{\tP}{\post}$, the \emph{Kullback-Leibler divergence} between $\tPX$ and $\post$, is always nonnegative,
we see that 
\begin{align}
	\lLH&\geq \LB(\tP,\Th) \label{eq:em_lh_lb}
\end{align}
with equality when 
\begin{align}
	\tPX &= \post \label{eq:em_maxq}
\end{align}
The nonnegativeness of the Kullback-Leibler divergence can be proved by
noting that $-\log$ is a convex function and so \emph{Jensen's inequality}
can be applied \parencite{Bishop2006}.

We are now ready the define the EM algorithm, which produces
a series of estimates $\{\hat{\Th}_j\}$ to the parameter $\Th$
starting from an initial guess $\hat{\Th}_0$. The two alternating
steps of the algorithm are:

\begin{enumerate}
  \item Given a current estimate $\hat{\Th}_j$ of the parameters, maximize
  $\LB(q,\hat{\Th}_j)$ with respect to the distribution $q$. As shown by equations 
  \eqref{eq:em_lh_lb} and \eqref{eq:em_maxq}, the maximum is obtained with 
  $q^*(\v{X}) = \Pdf{\v{X}}{\v{Y},\hat{\Th}_j}$, the posterior
  distribution of the states given the current parameter estimate. After the maximization
  we have
  	\begin{align}
  		L\!\left(\hat{\Th}_j\right)=\LB\!\left(\Pdf{\v{X}}{\v{Y},\hat{\Th}_j},\hat{\Th}_j\right).  
		\label{eq:LH_eq_LB}
	\end{align}
	This is the \emph{E-step} 
  \item Maximize $\LB\!\left(\Pdf{\v{X}}{\v{Y},\hat{\Th}_j},\Th\right)$ with respect
  to $\Th$ to obtain a new estimate $\hat{\Th}_{j+1}$. This is the M-step.
\end{enumerate}
We can then formulate a so called \emph{fundamental inequality of EM} \parencite{Cappe2005}:
\begin{align}
	L\left(\hat{\Th}_{j+1}\right) - L\left(\hat{\Th}_j\right)\geq & \LB\!\left(\Pdf{\v{X}}{\v{Y},\hat{\Th}_j},\hat{\Th}_{j+1}\right) - \LB\!\left(\Pdf{\v{X}}{\v{Y},\hat{\Th}_j},\hat{\Th}_{j}\right) 
	\label{eq:fundamental_inequality}
\end{align}
which is just the combination of \eqref{eq:em_lh_lb} and \eqref{eq:LH_eq_LB}. But it highlights
the fact that the likelihood is increased with every new estimate $\hat{\Th}_{j+1}$.
Also following from \eqref{eq:fundamental_inequality} is the fact that if the iterations
stop at a certain point, i.e. $\hat{\Th}_{l+1}=\hat{\Th}_l$ at iteration $l$, then
$\LB\!\left(\Pdf{\v{X}}{\v{Y},\hat{\Th}_l},\Th\right)$ must be maximal at $\hat{\Th}_l$
and so the gradients of the lower bound and of the likelihood must be zero. Thus
$\hat{\Th}_l$ is a \emph{stationary point} of $L(\Th)$, i.e a local maximum or a saddle point.



Another property of the lower bound worth stating formally is the following: assume that
the likelihood and \eqref{eq:KL} are continuously differentiable, then

\begin{align}
		\left.\dpd{\lLH}{\theta_i}\right|_{\Th=\hat{\Th}_j}&=\left.\dpd{\LB\!\left(\Pdf{\v{X}}{\v{Y},\hat{\Th}_j},\Th\right)}{\theta_i}\right|_{\Th=\hat{\Th}_j} \label{eq:EM_gradients}
\end{align}
This was implicitly clear already from \eqref{eq:LH_eq_LB} by remembering that $\LB$ is a lower
bound.

If we substitute $\Pdf{\v{X}}{\v{Y},\hat{\Th}}$ for $q$ in \eqref{eq:em_lb_q},
we get
\begin{align}
\begin{split}
	\LB(q,\Th)&=\defint{\v{X}}{}{\Pdf{\v{X}}{\v{Y},\hat{\Th}}\lcLH}{\v{X}} 
	-\defint{\v{X}}{}{\Pdf{\v{X}}{\v{Y},\hat{\Th}}\log\Pdf{\v{X}}{\v{Y},\hat{\Th}}}{\v{X}}\\
	&=\Lb(\Th,\hat{\Th})+C,
	\label{eq:completedata_loglikelihood}
\end{split}
\end{align}
where $C$ is a constant (the differential entropy of $\Pdf{\v{X}}{\v{Y},\hat{\Th}}$) and $\Lb(\Th,\hat{\Th})$ can be interpreted as the
expectation of the complete-data log-likelihood with respect to the posterior
distribution of the states given the current value of the parameter. 


\subsubsection{EM as a special case of variational Bayes}
\parencite{barber2012bayesian,jordan1998learning}
Variational Bayes (VB) is a fully Bayesian methodology where one seeks
for an approximation to the parameter posterior
\begin{align}
	\Pdf{\Th}{\Y}\propto \defint{\mathcal{X}}{}{\Pdf{\X,\Y}{\Th}}{\X}\Pdf{\Th}
	\label{tablelabel}
\end{align}
As mentioned earlier, finding this distribution is commonly intractable, so in VB
we assume a factorized form for the joint posterior of states and parameters
\begin{align}
	\Pdf{\X,\Th}{\Y}\approx \Pdf[q]{\X}\Pdf[q]{\Th}
	\label{eq:VB_factorization}
\end{align}
and the task is then to find the best approximation with respect
to the KL divergence between the true posterior and the approximation
\begin{align}
	\KL{\Pdf[q]{\X}\Pdf[q]{\Th}}{\Pdf{\X,\Th}{\Y}} &= \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} + \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} -
	\E{\Pdf{\X,\Th}{\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}}.
	\label{eq:KL_VB}
\end{align}
Using ${\Pdf{\X,\Th}{\Y}}=\Pdf{\X,\Th,\Y}/\Pdf{\Y}$ equation \eqref{eq:KL_VB} gives
\begin{align}
	\lLH &\geq \E{\Pdf{\X,\Th,\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}} - \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} -
	\E{\Pdf[q]{\X}}{\Pdf[q]{\X}}
	\label{eq:VB_bound}
\end{align}
and thus minimizing the KL divergence is equivalent to finding the tightest lower bound to
the log likelihood. Analogously to EM, minimizing the KL divergence is done iteratively
keeping $\Pdf[q]{\Th}$ fixed and minimizing w.r.t $\Pdf[q]{\X}$ in the ``E''-step
and vice versa in the ``M''-step:

\begin{description}
\addtolength{\leftskip}{1cm}
\item[E-step]
\begin{align}
	\Pdf[q^{\text{new}}]{\X}=\text{argmax}_{\Pdf[q]{\X}}\left(\KL{\Pdf[q]{\X}\Pdf[q^{\text{old}}]{\Th}}{\Pdf{\X,\Th}{\Y}}\right)
	\label{eq:VB_E}
\end{align}
\item[M-step]
\begin{align}
	\Pdf[q^{\text{new}}]{\Th}=\text{argmax}_{\Pdf[q]{\Th}}\left(\KL{\Pdf[q^{\text{new}}]{\X}\Pdf[q]{\Th}}{\Pdf{\X,\Th}{\Y}}\right)
	\label{eq:VB_M}
\end{align}
\end{description}


Let us then suppose that we only wish to find the MAP point estimate $\Th^*$. This can be accomplished
by assuming a delta function form $\Pdf[q]{\Th}=\delta\left(\Th,\Th^*\right)$ for the parameter factor in the
joint distribution of states and parameters \eqref{eq:VB_factorization}.
With this assumption equation \eqref{eq:VB_bound} becomes
\begin{align}
	\Pdf{\Y}{\Th^*} &\geq \E{\Pdf{\X,\Th^*,\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}} - \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} + \mathtt{const}
	\label{eq:VB_MAP_boundl}
\end{align}
and the ``M''-step \eqref{eq:VB_M} can then be written as
\begin{align}
	\Th^* &= \text{argmax}_{\Th}\left(\E{\lcLH}{\Pdf[q]{\X}}+\log\Pdf{\Th}\right).
	\label{tablelabel}
\end{align}
If the point estimate is plugged in the ``E''-step equation \eqref{eq:VB_E} we have
\begin{align}
	\Pdf[q^{\text{new}}]{\X}\propto \Pdf{\X,\Y}{\Th^*} \propto \Pdf{\X}{\Y,\Th^*} 
	\label{tablelabel}
\end{align}

\subsubsection{Partial E and M steps}




\subsubsection{M-step with structured matrices}
\cite{Wills2011}
When we want to maximize $\Lb(\Th,\hat{\Th})$ w.r.t some other
parameters than the ones in $\Th_M$, the situation becomes more complicated.
In the general case, no analytical formulas can be found. We therefore seek
to maximize $\Lb(\Th,\hat{\Th})$ numerically, analogously to how $L(\gv{\theta})$
was maximized in section~\ref{sec:grad}.

Fortunately calculating the gradient of $\Lb(\Th,\hat{\Th})$ is straightforward:

\begin{align}
\begin{split}
	-2\dpd{\Lb(\Th,\hat{\Th}))}{\theta_i}
	&=\Tr{-\v{Q}^{-1}\dpd{\v{Q}}{\theta_i}\v{Q}^{-1}
	\left(\v{B}_1-\v{A}\v{B}_2^T-\v{B}_2\v{A}^T+\v{A}\v{B}_3\v{A}^T\right)}\\
	&+\Tr{\v{Q}^{-1}\left(-\dpd{\v{A}}{\theta_i}\v{B}_2^T-\v{B}_2\dpd{\v{A}}{\theta_i}^T+\dpd{\v{A}}{\theta_i}\v{B}_3\v{A}^T+\v{A}\v{B}_3\dpd{\v{A}}{\theta_i}^T\right)}\\
	&+\Tr{-\v{R}^{-1}\dpd{\v{R}}{\theta_i}\v{R}^{-1}
	\left(\v{B}_4-\v{H}\v{B}_5^T-\v{B}_5\v{H}^T+\v{H}\v{B}_1\v{H}^T\right)}\\
	&+N\Tr{\v{Q}^{-1}\dpd{\v{Q}}{\theta_i}}
	+N\Tr{\v{R}^{-1}\dpd{\v{R}}{\theta_i}}\\
	\label{eq:dLB}
\end{split}
\end{align}



\subsubsection{M-step maximization equations}

Let us separate the set of all parameters $\Th$ into
subsets

\begin{align}
	\Th&=\left\{\gv{\psi},\gv{\omega},\v{Q},\v{R}\right\},
\end{align}
where $\gv{\psi}$ are the parameters of $f$ and $\gv{\omega}$ are
the parameters of $h$ (these sets can intersect). Let us first consider
the maximization with respect to $\v{Q}$. We have

\begin{multline}
	-2\dpd{\Lb(\Th,\hat{\Th}_j)}{\v{Q^{-1}}}=
	\sum_{k=1}^N\dpd{}{\v{Q^{-1}}}\Tr{\v{Q}^{-1}\E{\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)^T}{\hat{\Th}_j}}\\+N\dpd{}{\v{Q^{-1}}}\log\abbs{\v{Q}}
	\label{eq:clLH_pdQ}
\end{multline}
Using formula 92 in \cite{Petersen2008} for the first derivative and formula 51 for the second, we get
\begin{align}
	-2\dpd{\Lb(\Th,\hat{\Th}_j)}{\v{Q^{-1}}}&=
	\sum_{k=1}^N\E{\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)^T}{\hat{\Th}_j}-N\v{Q}
	\label{eq:clLH_pdQ2}
\end{align}
and setting this to zero we get the update equation for the next estimate of $\v{Q}$
\begin{align}
	\v{Q}_{j+1}&=\frac{1}{N}\sum_{k=1}^N\E{\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)^T}{\hat{\Th}_j}
\end{align}
The derivation of the update equation for the next estimate of $\v{R}$ is exactly analogous, giving
\begin{align}
	%-2\dpd{\Lb(\Th,\hat{\Th}_j)}{\v{R^{-1}}}&=\sum_{k=1}^N\E{\left(\v{y}_k-\v{g}(\v{x}_{k})\right)^T\left(\v{y}_k-\v{g}(\v{x}_{k})\right)}{\hat{\Th}_j}-N\v{R}\\
	\v{R}_{j+1}&=\frac{1}{N}\sum_{k=1}^N\E{\left(\v{y}_k-\v{h}(\v{x}_{k})\right)\left(\v{y}_k-\v{h}(\v{x}_{k})\right)^T}{\hat{\Th}_j}
\end{align} 



\subsubsection{Gradient computation}
In this case we have
\begin{align}
	\dpd{\Lb(\Th,\hat{\Th}))}{\theta_i}&=\dpd{I_1}{\theta_i}+\dpd{I_3}{\theta_i}+\dpd{I_3}{\theta_i}
	\label{eq:dLB_nonlinear_1}
\end{align}
and
\begin{align}
\begin{split}
	-2\dpd{I_2}{\theta_i}
	=&-\Tr{\v{Q}^{-1}\dpd{\v{Q}}{\theta_i}\v{Q}^{-1}
	\sum_{k=1}^N\E{\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)^T}{\hat{\Th}_j}}\\
	&-\Tr{\v{Q}^{-1}\sum_{k=1}^N\E{\dpd{\v{f}(\v{x}_{k-1})}{\theta_i}\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)^T+\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)\dpd{\v{f}^T(\v{x}_{k-1})}{\theta_i}}{\hat{\Th}_j}}\\
	&+N\Tr{\v{Q}^{-1}\dpd{\v{Q}}{\theta_i}}\\
	\dpd{I_3}{\theta_i}=&
	-\Tr{\v{R}^{-1}\dpd{\v{R}}{\theta_i}\v{R}^{-1}
	\sum_{k=1}^N\E{\left(\v{y}_k-\v{h}(\v{x}_{k})\right)\left(\v{y}_k-\v{h}(\v{x}_{k})\right)^T}{\hat{\Th}_j}}\\
	&-\Tr{\v{R}^{-1}\sum_{k=1}^N\E{\dpd{\v{h}(\v{x}_{k})}{\theta_i}\left(\v{y}_k-\v{h}(\v{x}_{k})\right)^T+\left(\v{y}_k-\v{h}(\v{x}_{k})\right)\dpd{\v{h}^T(\v{x}_{k})}{\theta_i}}{\hat{\Th}_j}}\\
	&+N\Tr{\v{R}^{-1}\dpd{\v{R}}{\theta_i}}\\
	\label{eq:dLB_nonlinear}
\end{split}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Applying EM}%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{EM in linear-Gaussian SSM:s}
\parencite{shumway1982approach,Ghahramani1996}
Continuing with the application of EM to the linear-Gaussian state space models, 
the complete-data log-likelihood function is now
\begin{align}
\label{eq:clLH}
\begin{split}
	L(\v{X},\Th)&=
	\frac{1}{2}\left(\v{x}_0-\gv{\mu}\right)^T\gv{\Sigma}^{-1}\left(\v{x}_0-\gv{\mu}\right)+\frac{1}{2}\log\abbs{\gv{\Sigma}}\\
	&+\frac{1}{2}\sum_{k=1}^N\left(\v{y}_k-\v{Hx}_k\right)^T\v{R}_{k|k-1}{-1}\left(\v{y}_k-\v{Hx}_k\right)+\frac{N}{2}\log\abbs{\v{R}}\\
	&+\frac{1}{2}\sum_{k=1}^N\left(\v{x}_k-\v{A}\v{x}_{k-1}\right)^T\v{Q}^{-1}\left(\v{x}_k-\v{A}\v{x}_{k-1}\right)+\frac{N}{2}\log\abbs{\v{Q}}\\
	&+C
\end{split}
\end{align}

We can then take the expectation of \eqref{eq:clLH}:
\begin{align}
&\Lb(\Th,\hat{\Th}_j)=\E{L(\v{X},\Th)}{\hat{\Th}_j}=\\
\begin{split}
	&\Tr{\gv{\Sigma}^{-1}\left(\v{P}_{0|N}+(\v{m}_{0|N}-\gv{\mu})(\v{m}_{0|N}-\gv{\mu})^T\right)}+\log\abbs{\gv{\Sigma}}\\
	+&\Tr{\v{Q}^{-1}\left(\sum_{k=1}^N\E{\v{x}_k\v{x}_{k}^T}-\v{A}\sum_{k=1}^N\E{\v{x}_k\v{x}_{k-1}^T}^T-\sum_{k=1}^N\E{\v{x}_k\v{x}_{k-1}^T}\v{A}^T+\v{A}\sum_{k=1}^N\E{\v{x}_{k-1}\v{x}_{k-1}^T}\v{A}^T\right)}+N\log\abbs{\v{Q}}\\
	+&\Tr{\v{R}^{-1}\left(\sum_{k=1}^N\v{y}_{k}\v{y}_{k}^T-\v{H}\sum_{k=1}^N\v{y}_{k}\E{\v{x}_{k}^T}^T-\sum_{k=1}^N\v{y}_{k}\E{\v{x}_{k}^T}\v{H}^T+\v{H}\sum_{k=1}^N\E{\v{x}_k\v{x}_{k}^T}\v{A}^T\right)}+N\log\abbs{\v{R}}\\
	\label{eq:lb}
\end{split}
\end{align}

Finally, the expectations in \eqref{eq:sum_expectations} can be
calculated with the combined use of the Kalman filter and the 
RTS smoother:

\begin{align}
	\E{\v{x}_k}&=\v{m}_{k|N}\\
	\E{\v{x}_k\v{x}_{k|k-1}T}&=\v{P}_{k|N}+\v{m}_{k|N}(\v{m}_{k|N})^T\\
	\E{\v{x}_k\v{x}_{k-1}^T}&=\v{C}_{k|N}+\v{m}_{k|N}(\v{m}_{k-1|N})^T,
\end{align}
where $\v{m}_{k|N}$ is the mean and $\v{P}_{k|N}$ is the variance of the state 
$\v{x}_k$ given the observations $\v{y}_1,\dots,\v{y}_N$.
For more specific details see \parencite{Gibson2005}. All in all, the E-step of the 
EM algorithm in linear-Gaussian SSM:s corresponds to computing
the matrices in \eqref{eq:sum_expectations} with the help of the Kalman filter and
the RTS smoother. In \parencite{Elliott1999} a new kind of filter is presented that
can compute \eqref{eq:sum_expectations} with only forward recursions. 


After having calculated the statistics \eqref{eq:sum_expectations} 
$\hat{\Th}$, we proceed to estimate the new
value $\Th^*$ by finding the maximum of
$\Lb(\Th,\hat{\Th})$ in the M-step. The complexity of this step
depends of the structure in $\Th_M$. In the case of no structure,
the M-step reduces to simple linear regression. Let us now derive the M-step
maximization formulas for $\v{A}$, $\v{Q}$, $\v{H}$ and $\v{R}$. 
To do that, we take the partial derivatives of $\Lb(\Th,\hat{\Th})$
and set them to zero. We get \parencite{Ghahramani1996}:

\begin{subequations}
\begin{align}
	\v{A}_{j+1}&=\left(\sum_{k=1}^N\E{\v{x}_k\v{x}_{k-1}^T}\right)\left(\sum_{k=1}^N\E{\v{x}_{k-1}\v{x}_{k-1}^T}\right)^{-1} \label{eq:A_new}\\
	\v{Q}_{j+1}&=\sum_{k=1}^N\E{\v{x}_k\v{x}_{k}^T}-\v{A}_{j+1}\left(\sum_{k=1}^N\E{\v{x}_k\v{x}_{k-1}^T}\right)^T \label{eq:Q_new}\\
	\v{H}_{j+1}&=\sum_{k=1}^N\v{y}_{k}\E{\v{x}_{k}^T}\left(\sum_{k=1}^N\E{\v{x}_k\v{x}_{k}^T}\right){-1} \label{eq:H_new}\\
	\v{R}_{j+1}&=\sum_{k=1}^N\v{y}_{k}\v{y}_{k}^T-\v{H}_{j+1}\left(\sum_{k=1}^N\v{y}_{k}\E{\v{x}_{k}^T}\right)^T \label{eq:R_new}
\end{align}
\end{subequations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EM in nonlinear-Gaussian SSM:s}%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As explained in section \ref{sec:nonlinear_state}, in then nonlinear case
the filtering and smoothing distributions cannot be computed exactly.
Thus the E-step solution is also only approximate and the convergence
guarantees of EM won't apply anymore. The proposed methods can nevertheless
provide good results most of the time. In the fortunate case that the
model is linear-in-the-parameters the M-step can be solved in closed form.
This situation will be covered later in section~\ref{sec:litp}.

Now the expectation of the complete-data log-likelihood becomes
\begin{align}
\begin{split}
	\Lb(\Th,\hat{\Th}_j)&=\v{I}_1+\v{I}_2+\v{I}_3
\end{split}
\label{eq:ecLH_nonlinear}
\end{align}
where
\begin{align}
\begin{split}
\v{I}_1&=\frac{1}{2}\E{\left(\v{x}_0-\gv{\mu}\right)^T\gv{\Sigma}^{-1}\left(\v{x}_0-\gv{\mu}\right)}{\hat{\Th}_j}+\log\abbs{\gv{\Sigma}}\\
&=\frac{1}{2}\E{\Tr{\gv{\Sigma}^{-1}\left(\v{x}_0-\gv{\mu}\right)\left(\v{x}_0-\gv{\mu}\right)^T}}{\hat{\Th}_j}+\log\abbs{\gv{\Sigma}}\\
&=\frac{1}{2}\Tr{\gv{\Sigma}^{-1}\E{\left(\v{x}_0-\gv{\mu}\right)\left(\v{x}_0-\gv{\mu}\right)^T}{\hat{\Th}_j}}+\log\abbs{\gv{\Sigma}}\label{eq:ecLH_nonlinear_1}
\end{split}\\
\v{I}_2&=\frac{1}{2}\Tr{\v{Q}^{-1}\sum_{k=1}^N\E{\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)\left(\v{x}_k-\v{f}(\v{x}_{k-1})\right)^T}{\hat{\Th}_j}}+N\log\abbs{\v{Q}}\label{eq:ecLH_nonlinear_2}\\
\v{I}_3&=\frac{1}{2}\Tr{\v{R}^{-1}\sum_{k=1}^N\E{\left(\v{y}_k-\v{h}(\v{x}_{k})\right)\left(\v{y}_k-\v{h}(\v{x}_{k})\right)^T}{\hat{\Th}_j}}+N\log\abbs{\v{R}}\label{eq:ecLH_nonlinear_3}
\end{align}
The expectations in \eqref{eq:ecLH_nonlinear} should in principle be taken
with respect to the joint posterior distribution
\begin{align}
	\Pdf{\v{x}_0,\dots,\v{x}_{N}}{\v{Y},\hat{\Th}_j}.
	\label{eq:joint_posterior_of_state}
\end{align}
To calculate equation~\eqref{eq:ecLH_nonlinear} we don't need the full
joint posterior of the states \eqref{eq:joint_posterior_of_state}, but
only the marginals with two consecutive states
\begin{align}
	\Pdf{\v{x}_{k-1},\v{x}_{k}}{\v{Y},\hat{\Th}_j},\;k=1,\dots,N,
	\label{eq:joint_posterior_of_consecutive_states}
\end{align}
which in general are not Gaussian anymore.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EM in linear-in-the-parameters SSM:s}%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:litp}

Suppose the function $\v{f}$ is linear in the parameters and the dimension of the state $\v{x}$ is $d$.
Then in the most general case $\v{f}(\v{x}):\R^d\to\R^d$ is a linear
combination of vector valued functions $\gv{\rho}_k(\v{x}):\R^d\to\R^{d_k}$  
and the parameters are matrices $\gv{\Phi}_k\in\R^{d\times d_k}$. More specifically,
we have

\begin{align}
\begin{split}
	\v{f}(\v{x})&=\gv{\Phi}\gv{\rho}_1(\v{x})+\dots+\gv{\Phi}_m\gv{\rho}_m(\v{x})\\
	&=
	\begin{bmatrix}
		\gv{\Phi}_1 & \dots & \gv{\Phi}_m
	\end{bmatrix}
	\begin{bmatrix}
		\gv{\rho}_1(\v{x})\\
		\vdots\\ 
		\gv{\rho}_m(\v{x})
	\end{bmatrix}\\
	&=\v{A}\v{g}(\v{x}),
\end{split}
\end{align}
where $\v{A}\in\R^{d\times\sum_{k=1}^m d_k}$ and $\v{g}(\v{x}):\R^d\to \R^{\sum_{k=1}^m d_k}$ . 
For example, in case of the function
\begin{align}
	f(x,t)&=ax+b\frac{x}{1+x^2}+c\cos(1.2t)
\end{align}
we would have
\begin{align}
	f(x,t)&=
	\begin{bmatrix}
		a & b & c
	\end{bmatrix}
	\begin{bmatrix}
		x\\
		\frac{x}{1+x^2}\\ 
		\cos(1.2t)
	\end{bmatrix}
\end{align}
Suppose now, that the matrix $A$ depends on parameters $\v{s}$.
Let us then deduce the maximization equation for the parameter $s_i$:
\begin{align}
	-2\dpd{\Lb(\Th,\hat{\Th}_j)}{s_i}&=
	\sum_{k=1}^N\E{\dpd{}{s_i}\left(\v{x}_k-\v{A}\v{g}(\v{x}_{k-1})\right)^T\v{Q}^{-1}\left(\v{x}_k-\v{A}\v{g}(\v{x}_{k-1})\right)}{\hat{\Th}_j}\label{eq:s_grad}
\end{align}
and
\begin{align}
	&\dpd{}{s_i}\left(\v{x}_k-\v{A}\v{g}(\v{x}_{k-1})\right)^T\v{Q}^{-1}\left(\v{x}_k-\v{A}\v{g}(\v{x}_{k-1})\right)\\
	=&-2\v{g}(\v{x}_{k-1})^T\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\left(\v{x}_k-\v{A}\v{g}(\v{x}_{k-1})\right)\\
	=&-2\Tr{\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\v{x}_k\v{g}(\v{x}_{k-1})^T}-2\Tr{\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\v{A}\v{g}(\v{x}_{k-1})\v{g}(\v{x}_{k-1})^T}\\
	=&-2\Tr{\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\left(\v{x}_k\v{g}(\v{x}_{k-1})^T-\v{A}\v{g}(\v{x}_{k-1})\v{g}(\v{x}_{k-1})^T\right)}\label{eq:a_grad}.
\end{align}
Combining equations \eqref{eq:a_grad} and \eqref{eq:s_grad} then gives
\begin{multline}
	\dpd{\Lb(\Th,\hat{\Th}_j)}{s_i}=\\
	-2\Tr{\dpd{\v{A}}{s_i}^T\v{Q}^{-1}\left(\sum_{k=1}^N\E{\v{x}_k\v{g}(\v{x}_{k-1})^T}-\v{A}\sum_{k=1}^N\E{\v{g}(\v{x}_{k-1})\v{g}(\v{x}_{k-1})^T}\right)}
	\label{eq:pdcLH_nonlinear}
\end{multline}
If we then set \eqref{eq:pdcLH_nonlinear} to zero, we have
\begin{align}
	\v{A}_{j+1}=\left(\sum_{k=1}^N\E{\v{x}_k\v{g}(\v{x}_{k-1})^T}\right)\left(\sum_{k=1}^N\E{\v{g}(\v{x}_{k-1})\v{g}(\v{x}_{k-1})^T}\right)^{-1}
\end{align}
which is similar to \eqref{eq:A_new}

Suppose the function $\v{h}$ is linear in the parameters and the dimension of
the state $\v{x}$ is $d^x$ and of the measurements $d^y$.
Then in the most general case $\v{h}(\v{x}):\R^{d^x}\to\R^{d^y}$ is a linear
combination of vector valued functions $\gv{\pi}_k(\v{x}):\R^{d^x}\to\R^{d_k}$  
and the parameters are matrices $\gv{\Upsilon}_k\in\R^{d^x\times d_k}$. More
specifically, we have

\begin{align}
\begin{split}
	\v{h}(\v{x})&=\gv{\Upsilon}\gv{\pi}_1(\v{x})+\dots+\gv{\Upsilon}_m\gv{\pi}_m(\v{x})\\
	&=
	\begin{bmatrix}
		\gv{\Upsilon}_1 & \dots & \gv{\Upsilon}_m
	\end{bmatrix}
	\begin{bmatrix}
		\gv{\pi}_1(\v{x})\\
		\vdots\\ 
		\gv{\pi}_m(\v{x})
	\end{bmatrix}\\
	&=\v{H}\v{l}(\v{x}),
\end{split}
\end{align}
  
Suppose now, that the matrix $\v{H}$ depends on parameters $\v{t}$.
Let us then deduce the maximization equation for the parameter $t_i$:
\begin{align}
	-2\dpd{\Lb(\Th,\hat{\Th}_j)}{t_i}&=
	\frac{1}{2}\Tr{\v{R}^{-1}\sum_{k=1}^N\E{\dpd{}{t_i}\left(\v{y}_k-\v{H}\v{l}(\v{x}_k)\right)\left(\v{y}_k-\v{H}\v{l}(\v{x}_k)\right)^T}{\hat{\Th}_j}}
\label{eq:s_grad}
\end{align}
and

If we then set \eqref{eq:pdcLH_nonlinear} to zero, we have
\begin{align}
	\v{H}_{j+1}=\left(\sum_{k=1}^N\v{y}_k\E{\v{l}(\v{x}_{k})^T}\right)\left(\sum_{k=1}^N\E{\v{l}(\v{x}_{k})\v{l}(\v{x}_{k})^T}\right)^{-1}
\end{align}
which is similar to \eqref{eq:H_new}


\subsection{Comparisons}
\subsubsection{Convergence}
\subsubsection{Computational complexity}
\parencite{Harvey1990,Watson1983,Cappe2005,Saatci2011,Olsson2007,Salakhutdinov2003a}




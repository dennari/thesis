%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Maximum likelihood and maximum a posteriori estimation}%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the Bayesian sense the complete answer to the parameter estimation
problem is the marginal posterior probability of the parameters
given the measurements, which is given by Bayes' rule as
\begin{align}
	\Pdf{\gv{\theta}}{\Y}&=\frac{\Pdf{\Y}{\gv{\theta}}\Pdf{\gv{\theta}}}{\Pdf{\Y}}.
	\label{eq:param_post}
\end{align}

Computing the posterior distribution of the parameters is usually intractable \todo{why? examples?}. A much
easier problem is finding a suitable \emph{point estimate} $\hat{\gv{\theta}}$.
This effectively means that we don't need to worry about the normalizing
term $\Pdf{\Y}$, since it's constant with respect to the parameters. 
A point estimate that maximizes the posterior distribution
is called a \emph{maximum a posteriori} (MAP) estimate. 
Since the logarithm is a strictly monotonic function, maximizing a function
is the same as maximizing its logarithm. Thus the MAP estimate $\Th^*$ is given by 
\begin{align}
	\Th_{\text{MAP}} &= \argmax_{\Th}\brak[\big]{\underbrace{\log \Pdf{\Y}{\gv{\theta}}}_{\Pdf[\ell]{\Th}} + \log\Pdf{\gv{\theta}}}
	\label{eq:MAP}
\end{align}

In the case of a flat (constant and thus improper)
prior distribution, $\Pdf{\Th}$, the MAP estimate converges to the
\emph{maximum likelihood} (ML) estimate
\begin{align}
	\Th_{\text{ML}} &= \argmax_{\Th}\brak*\lLH
	\label{eq:ML}
\end{align}
Going further we we will only be concerned with finding the ML estimate, but it should
be remembered that both of the methods we consider can be extended
to the estimation of the MAP estimate in a straightforward fashion.
\todo{Explain MAP in both cases}

Among different point estimates, the maximum likelihood estimator has good statistical properties.
Let us denote the true parameter value, the value that the data was generated with, with $\Th_\star$ and 
let $T$ denote the amount of observations.
Then provided that some conditions of not very restricting nature hold, we can state the following asymptotic properties 
for the ML estimate $\Th_{\text{ML}}$:\todo{Modify to reflect p.465 in Cappé}
\begin{description}
\addtolength{\leftskip}{1cm}
\item[Strong consistency]\hfill\\
An important property for an estimator, which says that
the estimator tends to the true value as the amount of data tends to infinity:
\begin{align}
	%\forall \Th \in \Theta\quad \frac{1}{n}\Pdf[\ell_n]{\Th} \xrightarrow{\mathrm{a.s.}} \lLH, \mathrm{when} n\to\infty
	\F{\ell_T}{\Th_{\text{ML}}} \xrightarrow{\mathrm{a.s.}} \F{\ell}{\Th_\star},\quad \mathrm{when}\;T\to\infty,
\end{align}
where $\ell_T$ is the likelihood function after $T$ measurements and $\ell$ is a continuous
deterministic function with a unique global maximum at $\Th_\star$.
%where $\Pdf[\ell_n]{\Th}$ is the log-likelihood given $n$ observations and $\lLH$ is a continuous deterministic
%function with a unique global maximum at $\Th_\star$.
\item[Asymptotic normality]\hfill\\
This property gives us the means to compute asymptotic error bounds for
the estimate:
\begin{align}
	\sqrt{T}\left(\Th_{\text{ML}}-\Th_\star\right) \xrightarrow{D} \N{\v{0}}{\F{\mathcal{I}^{-1}}{\Th_\star}},\quad \mathrm{when}\;T\to\infty,	
\end{align}
where $\F{\mathcal{I}}{\Th_\star}$ is the \emph{Fisher information matrix} evaluated at $\Th_\star$ 
\item[Efficiency]\hfill\\
When the amount of information tends to infinity, the ML-estimate achieves
the Cramér-Rao lower bound, i.e. no other consistent estimator has lower asymptotic mean-squared-error.
\end{description}


\subsubsection{Identifiability}

Intuitively, any parameters $\Th,\:\Th' \in \Theta$ cannot be distinguished
from each other with maximum likelihood estimation if
\begin{align}
	\Pdf{\Y}{\Th}&=\Pdf{\Y}{\Th'},
\end{align}
i.e., if the same data can arise with two (or more) separate
parameter values. Let us then go through the second order sufficient conditions
for a point $\Th_\star$ to be a local maximum. We can define a local maximum
to be a point $\Th_\star$, for which $\lLH[\Th_\star]-\lLH[\Th_\star+\v{d}] \geq 0$ for small $\v{d}$.

\begin{proposition}\label{prop:cond_for_max}
Let $\ell:\Theta \to \R$ have continuous second order partial derivatives and let $\Th_\star$
belong to the interior of $\Theta$. If
\begin{enumerate}[i)] \addtolength{\leftskip}{1cm} \itemsep1pt \parskip0pt \parsep0pt
  \item $\nabla\lLH[\Th_\star] =0$ 
  \item $\nabla^2\lLH[\Th_\star]$ is negative definite
\end{enumerate}
then $\Th_\star$ is a strict local maximum of $\ell$.
\end{proposition}
 \begin{proof}
 Since \textit{ii)}, there must exist a nonnegative number $a$ for which 
 $-\v{d}^\tr\nabla^2\lLH[\Th_\star]\v{d} \geq a\abs{\v{d}}^2$.
 Taylor expanding around $\Th_\star$ gives
 \begin{align*}
	\lLH[\Th_\star+\v{d}] &= \lLH[\Th_\star]+\nabla\lLH[\Th_\star]^\tr\v{d}+\frac{1}{2}\v{d}^\tr\nabla^2\lLH[\Th_\star]\v{d}+\F[\big]{o}{\,\abs{\v{d}}^2}\\
	\lLH[\Th_\star]-\lLH[\Th_\star+\v{d}] &= -\frac{1}{2}\v{d}^\tr\nabla^2\lLH[\Th_\star]\v{d}-\F[\big]{o}{\,\abs{\v{d}}^2}\\
	&\geq \frac{1}{2}a\abs{\v{d}^2}-\F[\big]{o}{\,\abs{\v{d}}^2}.
\end{align*}
For small enough $\abs{\v{d}}$, the right hand side of the last row must be nonnegative.
 \end{proof}

\todo{Elaborate on identifiability}
\parencite{Haykin2001,Cappe2005}

\subsubsection{Ascent methods}

Both of the parameter estimation methods we are going
discuss, the expectation maximization algorithm and
the instances of gradient based nonlinear programming dealt with in the
next chapter, belong to the class of \emph{iterative ascent methods} \parencite{luenberger2008}.
They are iterative since they produce a series of estimates, where
always the next estimate is based on the previous estimates and
the value of the objective function is increased with every new estimate.

An important theoretical result concerning iterative ascent methods is known
as the \emph{global convergence theorem} \parencite{luenberger2008}. It states
the necessary conditions that the estimate produced by an iterative ascent method
is a local maximum. Suppose then that $\ell:\Theta\to\R$ is the objective function
and we are trying to solve the \emph{unconstrained} optimization problem
\begin{align}
	\Th_\star&=\argmax_{\Th}\lLH,\quad\Th\in\Theta=\R^{d_\theta},
	\label{eq:optimization_problem}
\end{align}
where $\Th_\star$ need not be unique, i.e. there can be a solution set 
\begin{align}
	\mathcal{S}=\brac*{\Th_\star\in\Theta : \lLH[\Th_\star]\geq \lLH[\Th] \; \forall \Th\in\Theta}
\end{align}
To formulate the conditions under which this problem can be solved by an iterative ascent method,
we need to define the concept of a closed point-to-set mapping. $T$ is a point-to-set map
from $X$ to $Y$, if it maps points in $X$ to subsets of $Y$.
$T$ is said to be \emph{closed} at $\x \in X$ if
\begin{alignat*}{3}
	\mathrm{(i)}&\; & \x_j & \to \x,&\;&\x_j \in X\\
	\mathrm{(ii)} && \y_j &\to \y, && \y_j\in\F{T}{\x_j}\\
\shortintertext{imply that}
	&& \y&\in\F{T}{\x}	&&
\end{alignat*}
This can be considered a generalization of continuity.
\begin{theorem}
Given an initial value $\Th_0$, let the point-to-set map $\EMM*:\Theta\to Y$ construct a sequence
of estimates $\brac[\big]{\Th_{j+1}\in\Theta : \Th_{j+1}\in\EMM{\Th_j}}$ and let $\mathcal{S}\subset\Theta$ denote the solution
set. 
Then given the conditions
\begin{enumerate}[i)] \addtolength{\leftskip}{1cm} \itemsep1pt \parskip0pt \parsep0pt
	\item all $\Th_j$ belong to $\Theta_c$,  a compact subset of $\Theta$
	\item $\lLH$ is a continuous function in $\Theta$ with
\begin{eqspace}{5pt}{5pt}{1pt}{5pt}
\begin{align*}
	\Th_j \not\in \mathcal{S} \Rightarrow \lLH[\Th_{j+1}] > \lLH[\Th_j]\\  
	\Th_j \in \mathcal{S} \Rightarrow \lLH[\Th_{j+1}] \geq \lLH[\Th_j]  
\end{align*}
\end{eqspace}
	\item $\EMM*$ is closed at $\Theta\setminus\mathcal{S}$
\end{enumerate}
it must follow that the limit of every subsequence of $\brac*{\Th_j}$ is part of $\mathcal{S}$ 
\end{theorem}
As pointed out in \textcite{luenberger2008}, it is usually the third condition
of the above theorem that produces difficulties with actual implementations
of iterative ascent algorithms. We will discuss this point in further detail in connection
with the specific methods we're considering.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient based nonlinear optimization}\label{sec:grad}%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

There exists a large amount of efficient nonlinear optimization,
also called nonlinear programming, methods that require the gradient of the 
objective function to be available \parencite{luenberger2008}.
The best known general purpose algorithms probably belong to the 
classes of quasi-Newton or conjugate gradient methods. 
For example, MATLAB Optimization Toolbox contains the function
\texttt{fminunc} utilizing both conjugate gradient and 
quasi-Newton methods in certain cases \parencite{optimizationtoolbox2012}.
Optimization methods are commonly formulated for finding the minimum of the
objective function, so that we will present the canonical formulations and note
that maximizing $\lLH$ is the same as minimizing $-\lLH$.

The simplest gradient based method is the \emph{method of steepest ascent}.
It requires that the first partial derivatives of the objective function are defined
and continuous in their domain. The method of steepest ascent is then defined
by the iteration
\begin{align}
	\Th_{j+1}&=\Th_j+\alpha_j\nabla\lLH[\Th_j].
	\label{eq:steepest_descent}
\end{align}
The idea is intuitive since it is well known that the gradient
points to the direction of steepest ascent, i.e. to the direction
that is orthogonal to the isolines of constant value.
To determine $\alpha_j$, the \emph{step size}, another minimization problem needs to be solved,
namely
\begin{align}
	\alpha_j &= \argmin_\alpha \lLH[\Th_j+\alpha\nabla\lLH[\Th_j]].
	\label{eq:line_search}
\end{align}
These one dimensional optimization algorithms that are used
to solve the step-sizes are known as \emph{line search} methods \parencite{luenberger2008}.
Common line search methods include the golden rule method and 
methods based on polynomial interpolation.

We define the \emph{order of convergence} as the supremum of the
numbers $p\geq 0$, where
\begin{align}
	0\geq \bar{\lim_{j\to\infty}}\frac{\abs{\Th_{j+1}-\Th_\star}}{\abs{\Th_{j}-\Th_\star}^p} < \infty.
	\label{eq:order_of_convergence}
\end{align}
When $p=1$, we also define the \emph{linear rate of convergence} as
the number $0 \leq \rho < 1$ in
\begin{align}
	\lim_{j\to\infty}\frac{\abs{\Th_{j+1}-\Th_\star}}{\abs{\Th_{j}-\Th_\star}} = \rho.
	\label{eq:rate_of_convergence}
\end{align}
It can be shown the steepest ascent method has linear order of convergence ($p=1$)
and if the Hessian of the objective function is positive definite with
$r=A/a$, the ratio of the largest and smallest eigenvalues, 
\begin{align}
	\lim_{j\to\infty}\frac{\abs{\Th_{j+1}-\Th_\star}}{\abs{\Th_{j}-\Th_\star}} \leq \left(\frac{r-1}{r+1}\right)^2.
\end{align}
A much more efficient nonlinear optimization algorithm is the \emph{Newton's method}.
It is based on Taylor expanding the objective function around the
current estimate $\Th_j$. Assume that $\ell$ has continuous second-order partial derivatives. Then
\begin{align}
	\lLH[\Th] &\approx \lLH[\Th_j]+\nabla\lLH[\Th_j]^\tr\fparen*{\Th_j-\Th}+\frac{1}{2}\fparen*{\Th_j-\Th}^\tr\nabla^2\lLH[\Th_j]\fparen*{\Th_j-\Th}
	\label{eq:second_order_expansion}
\end{align}
and then maximizing the approximation by setting its gradient to zero
\begin{align}
	\nabla\lLH[\Th_j]-\nabla^2\lLH[\Th_j]\fparen*{\Th_j-\Th} &= 0\\
	\Rightarrow \Th_{j+1} &= \Th_j+\nabla^2\lLH[\Th_j]^{-1}\nabla\lLH[\Th_j].
	\label{eq:newton_gradient}
\end{align}
Per proposition~\ref{prop:cond_for_max}, near $\Th_\star$ the Hessian is 
invertable and so the algorithm is well defined there. It can be shown
that when initialized sufficiently close to $\Th_\star$, (pure form)
Newton's method always converges to $\Th_\star$ with order of convergence
at least \emph{two}.
 
Further away from the maximum, there are various problems with Newton's method as formulated
in equation~\eqref{eq:newton_gradient}. There are no guarantees for the invertability
of the Hessian and higher order terms may cause a step to actually decrease the objective
function. Thus we turn our attention to algorithms of the general form
\begin{align}
	\Th_{j+1} &= \Th_j+\alpha_j\v{D}_j\nabla\lLH[\Th_j],
	\label{eq:gradient_method_general}
\end{align}
where $\v{D}_j$ is a symmetric matrix, the \emph{search direction} is $\v{D}_j\nabla\lLH[\Th_j]$ and
the step-size is $\alpha_j$. Generally $\v{D}_j$ should also be negative definite, to guarantee
that the method is an ascent method for small $\alpha_j$ (as in proposition \eqref{prop:cond_for_max}).
  
Clearly we get gradient ascent with $\v{D}_j=\v{I}$
and Newton's method with  $\v{D}_j=\nabla^2\lLH[\Th_j]^{-1}$. 
Other methods of this form have thus orders of convergence
between one and two. In practice the step
size parameter is always determined by a line-search, so that different
algorithms of the form \eqref{eq:gradient_method_general} differ only in how
the search direction is computed. Even if we could guarantee the invertability of the Hessian, its computation is
nevertheless notoriously computationally demanding. For that reason, one should
in practice use \emph{quasi-Newton} methods, where the different kinds of
approximations are used for the inverse Hessian. Probably the most commonly
applied quasi-Newton method is the \emph{Broyden-Fletcher-Goldfarb-Shanno} (BFGS)
one. It has the appealing properties that only gradient information is needed
and it can guarantee ascent of the objective function on every step. We will not
go deeper into details, but a clear description can be found in \textcite{luenberger2008}.
Let us finally point out that the commonly used MATLAB unconditional nonlinear
optimization function \texttt{fminunc} that we referred to earlier, uses
the BFGS quasi-Newton method with cubic interpolation line search.


Let us then focus on computing the gradient of the
log-likelihood function $\lLH$, also known as the \emph{score function}.
By marginalizing the joint distribution of equation~\eqref{eq:joint_per_kalmanstep}
we get 
\begin{align}
	\Pdf{\y_k}{\y_{1:k-1},\Th}&=\N[\yk]{\v{H}\m_{k|k-1},\v{S}_k }.
\end{align}
Applying equation~\eqref{eq:lh_factorization} and taking the logarithm then gives
\begin{align}
	\lLH&=-\frac{1}{2}\sum_{k=1}^T\log\abbs{\v{S}_k}
	-\frac{1}{2}\sum_{k=1}^T\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)^\tr\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)+C,
	\label{eq:logLH}
\end{align}
where $C$ is a constant that doesn't depend on $\gv{\theta}$ and thus can
be ignored in the maximization.
There are exists two seemingly quite different methods for computing
the gradient of $\lLH$. The first one proceeds straightforwardly by taking the
partial derivatives of $\lLH$. As will soon be demonstrated, this leads
to some additional recursive formulas which allow computing
the gradient in parallel with the Kalman filter. The second method needs
the smoothing distributions with the cross-timestep covariances
and it can be easily computed with the expectation maximization machinery
that will be introduced later. These two methods can be proved to compute
the exact same quantity. At this point we will focus on the first one. Going further
it will be assumed that $\lLH$ is continuous and differentiable for all $\Th\in\Theta$.

In order to calculate the score function
\begin{align}
	\score[\Th']&=\eval{\dpd{\lLH}{\Th}}_{\Th=\Th'}
	=\eval{\bm{\dpd{\lLH}{\theta_1} & \dots & \dpd{\lLH}{\theta_{d_\theta}}}^\tr}_{\Th=\Th'},
	\label{eq:score}
\end{align}
we have to compute the partial
derivatives:
\begin{align}
\begin{split}
	\dpd{\lLH}{\theta_i}
	=&-\frac{1}{2}\sum_{k=1}^T\mathrm{Tr}\left(\v{S}_{k}^{-1}\dpd{\v{S}_k}{\theta_i}\right)\\
	&+\sum_{k=1}^T\left(\v{H}_k\dpd{\v{m}_{k|k-1}}{\theta_i}\right)^\tr\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)\\
	&+\frac{1}{2}\sum_{k=1}^T\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)^\tr\v{S}_{k}^{-1}\left(\dpd{\v{S}_k}{\theta_i}\right)\v{S}_{k}^{-1}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)\\
	\label{eq:dlogLH}
\end{split}
\end{align}
From the Kalman filter recursions \eqref{eq:Kalman_filter} we find out that 
\begin{align}
	\dpd{\v{S}_k}{\theta_i}&=\v{H}\dpd{\v{P}_{k|k-1}}{\theta_i}\v{H}+\dpd{\v{R}}{\theta_i}
\end{align}
so that we're left with the task of determining the partial derivatives for
$\v{m}_{k|k-1}$ and $\v{P}_{k|k-1}$:
\begin{align}
	\dpd{\v{m}_{k|k-1}}{\theta_i}&=\dpd{\v{A}}{\theta_i}\v{m}_{k-1|k-1}+\v{A}\dpd{\v{m}_{k-1|k-1}}{\theta_i} \label{eq:m_pred_pd}\\
	\begin{split}
	\dpd{\v{P}_{k|k-1}}{\theta_i}&=\dpd{\v{A}}{\theta_i}\v{P}_{k-1|k-1}\v{A}^\tr+\v{A}\dpd{\v{P}_{k-1|k-1}}{\theta_i}\v{A}^\tr\\
	&+\v{A}\v{P}_{k-1|k-1}\left(\dpd{\v{A}}{\theta_i}\right)^\tr+\dpd{\v{Q}}{\theta_i} \label{eq:P_pred_pd}
	\end{split}
\end{align}
as well as for $\v{m}_{k|k}$ and $\v{P}_{k|k}$:
\begin{align}
	\dpd{\v{K}_k}{\theta_i}&=\dpd{\v{P}_{k|k-1}}{\theta_i}\v{H}^\tr\v{S}_{k}^{-1}-\v{P}_{k|k-1}\v{H}^\tr\v{S}_{k}^{-1}\dpd{\v{S}_k}{\theta_i}\v{S}_{k}^{-1}
	\label{eq:K_pd}\\
	\dpd{\v{m}_{k|k}}{\theta_i}&=\dpd{\v{m}_{k|k-1}}{\theta_i}+\dpd{\v{K}_k}{\theta_i}\left(\v{y}_k-\v{H}\v{m}_{k|k-1}\right)-\v{K}_k\v{H}\dpd{\v{m}_{k|k-1}}{\theta_i}
	\label{eq:m_pd}\\
	\dpd{\v{P}_{k|k}}{\theta_i}&=\dpd{\v{P}_{k|k-1}}{\theta_i}-\dpd{\v{K}_k}{\theta_i}\v{S}_{k}\v{K}_{k}^\tr-\v{K}_{k}\dpd{\v{S}_k}{\theta_i}\v{K}_{k}^\tr-\v{K}_{k}^\tr\v{S}_{k}\left(\dpd{\v{K}_k}{\theta_i}\right)^\tr
	\label{eq:P_pd}
	\end{align}
Equations \eqref{eq:m_pred_pd}, \eqref{eq:P_pred_pd}, \eqref{eq:K_pd}, \eqref{eq:m_pd} and \eqref{eq:P_pd} together specify
a recursive algorithm for computing \eqref{eq:dlogLH} that can be run alongside the Kalman filter recursions.
As noted in \textcite{Cappe2005}, these equations are sometimes known as the \emph{sensitivity equations}
and they are derived at least in \textcite{Gupta1974} and \textcite{Mbalawataa}.

\subsubsection{Newton's method}

\subsubsection{Quasi-Newton methods}

\subsubsection{Convergence properties}

\subsubsection{Computational load}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Expectation maximization (EM)}%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Expectation maximization (EM) algorithm is a general
method for finding ML and MAP estimates in probabilistic models with missing data or
latent variables. It was first introduced in
the celebrated article of \textcite{Dempster1977}. Instead of maximizing
\eqref{eq:logLH} directly, EM alternates between computing a variational lower bound and then maximizing this bound
\parencite{Bishop2006,barber2012bayesian}. As will be seen, since the bound is strict, increasing the bound implies
an increase in the objective function. For these reasons EM is well described as lower bound optimization \parencite{Minka1998}.
We shall use $\E{\cdot}_{q}\equiv\defint{}{}{\cdot\,\Pdf[q]{z}}{z}$ to denote the expectation
over any distribution $\Pdf[q]{z}$.

Let us introduce a family of ``variational'' distributions 
indexed by the parameter $\gv{\psi}$, $\tPX$, over the states $\X$ (or the latent variables in general).
Noting now that $\Pdf{\X}{\Y,\Th}=\cLH/\LH$ and that $\lLH\equiv\log\LH$ is independent of $\X$ we can then perform the
following decomposition on the log likelihood:
\begin{align}
	\lLH &= \log\cLH - \log\post \nonumber\\
	&= \E{\log\cLH}_{\tPX} - \E{\log\post}_{\tPX} \nonumber\\ 
	&= \underbrace{\E{\log\cLH}_{\tPX} - \E{\tPX}_{\tPX}}_{\displaystyle \EMB{\gv{\psi}}{\Th}}+\KL{\tPX}{\post}. 
	\label{eq:lLH_decomp}
\end{align}
By invoking the nonnegativeness of
the \emph{Kullback-Leibler divergence}
\begin{align}
		\KL{\tPX}{\post} = -\E{\log{\frac{\post}{\tPX}}}_{\tPX},  
\end{align}
or equivalently the relation
\begin{align}
	\E{\log\tPX}_{\tPX} &\geq \E{\log\post}_{\tPX},	
\end{align}
provable by \emph{Jensen's inequality}, 
we see that $\EMB{\gv{\psi}}{\Th}$ is indeed a lower bound on $\lLH$. 

The \emph{sharpest} bound can clearly be found among
distributions of the form $\post{\Th'}$, since the Kullback-Leibler divergence vanishes with $\tPX=\post$.
For this reason, we will set
\begin{align}
	\tPX &= \post{\Th'}
\end{align} 
and denote
\begin{align}
	\EMQ &\equiv \E{\log\cLH}_{\post{\Th'}} \label{eq:EM_Q}\\
	\EMH &\equiv \E{\log\post}_{\post{\Th'}} \label{eq:EM_H}\\
	\EMB &= \EMQ - \EMH{\Th'}{\Th'} \label{eq:EM_B}
\end{align}
According to \eqref{eq:lLH_decomp} we now have
\begin{align}
	\lLH & \geq \EMB \quad \forall \Th,\Th' \in \Theta
	\label{eq:EM_LB}
\end{align}
and especially
\begin{align}
	\lLH & = \EMB{\Th}{\Th}
	\label{eq:EM_sharp_bound}
\end{align}
When we want to maximize $\EMB$ with respect to $\Th$
clearly it suffices to consider
only $\EMQ$, known as the \emph{expected complete-data log-likelihood}
or the \emph{intermediate quantity of EM}.

What is also interesting about $\EMQ$ is that it can be used to compute the gradient of the log-likelihood, i.e
the score, itself. From equations \eqref{eq:EM_B}
and \eqref{eq:EM_sharp_bound} it can be seen rather easily, that  
the score evaluated at $\widehat{\Th}$ is given by
\begin{align}
		\eval{\dod{\lLH}{\Th}}_{\Th=\widehat\Th}&=
		\eval{\dpd{ \EMQ{\widehat\Th}{\Th}}{\Th}}_{\Th=\widehat\Th} \label{eq:EM_gradients}
\end{align} 
Equation \eqref{eq:EM_gradients} is known as \emph{Fisher's identity} \parencite{Cappe2005}. It gives
an alternative route for the score function computation.
\todo{elaborate on score computation}
 

We are now ready the define the EM algorithm, 
which produces a series of estimates $\brac{\Th_j}$
starting from an initial guess $\Th_0$. The two alternating
steps of the algorithm are:

\begin{description}
\addtolength{\leftskip}{1cm}
  \item[E-step]\hfill\\
  Given the current estimate $\Th_j$ of the parameters, compute	
  $\post{\Th_j}$, the joint posterior distribution of the states. 
  As will be seen later, only certain marginals of this distribution
  are actually needed.  
  \item[M-step]\hfill\\ 
  Set
\todo{Bring in MAP estimation}
    \begin{align}
		\Th_{j+1}&=\argmax_{\Th}\EMQ{\Th_j}{\Th} \label{eq:EM_M}
	\end{align}
\end{description}
We are now in a position to formulate the so called \emph{fundamental inequality of EM} \parencite{Cappe2005}.
From \eqref{eq:EM_LB} we have
\begin{align*}
	\lLH[\Th_{j+1}] & \geq \EMQ{\Th_j}{\Th_{j+1}} - \EMH{\Th_j}{\Th_{j}}
\end{align*}
and using \eqref{eq:EM_sharp_bound} and \eqref{eq:EM_M} we can write
\begin{align}
\label{eq:fundamental_inequality}	
\begin{split}	
	\lLH[\Th_{j+1}] - \lLH[\Th_j] & \geq \EMQ{\Th_j}{\Th_{j+1}} - \EMQ{\Th_j}{\Th_{j}} \geq 0\\
\end{split}
\end{align}
This highlights
the fact that \emph{the likelihood is increased or unchanged with every new estimate} $\Th_{j+1}$.
Also following from \eqref{eq:fundamental_inequality} is the fact that if the iterations
stop at a certain point, i.e. $\Th_{l}=\Th_{l-1}$ at iteration $l$, then
$\EMQ{\Th_l}{\Th}$ must be maximal at $\Th=\Th_l$ and thus its gradient, and 
by \eqref{eq:EM_gradients} that of the likelihood, must be zero at $\Th=\Th_l$. Thus
$\Th_l$ is a \emph{stationary point} of $\lLH$, i.e a local maximum or a saddle point.

\subsubsection{EM in exponential families of distributions}

Computing the intermediate quantity of EM is especially simple
if the dynamic model and the measurement model belong to an exponential
family of distributions, which have probability distribution functions of the form 
\begin{align}
	\Pdf[q]{\z}{\Th}&=\F{h}{\z}\exp\brac[\big]{ \F{\gv{\psi}}{\Th}^\tr \F*{s}{\z}-\F{c}{\Th}}.
	\label{eq:exp_family}
\end{align}
Here $\F*{s}{\z}$ is called the vector of \emph{natural sufficient statistics} and
$\gv{\eta}\equiv \F{\gv{\psi}}{\Th}$ is the \emph{natural parameterization}.
Let us suppose now that the complete-data likelihood is of the form \eqref{eq:exp_family}, so
that $\v{z}^\tr=\brak*{\mathrm{vec}\brac*{\X}^\tr,\mathrm{vec}\brac*{\Y}^\tr}$, where the operator $\mathrm{vec}\brac*{\cdot}$
creates vectors out of matrices by stacking their columns, contains the
hidden variables $\X$ and the measurements $\Y$. 

The intermediate quantity, i.e the expectation of the logarithm of $\Pdf[q]{\z}{\Th}$ over the posterior distribution of 
$\X$ (implied) becomes now
\begin{align}
	\EMQ&=	\F{\gv{\psi}}{\Th}^\tr \E{\F*{s}{\z}}-\F{c}{\Th}+\E{\F{h}{\z}}.
\end{align}
Since the last term is independent of $\Th$ then the maximization in the M-step
is independent of this last term. Thus the role of the E-step degenerates into computing the
expectation of the sufficient statistics $\E{\F*{s}{\z}}$.


\subsubsection{EM as a special case of variational Bayes}
\todo{Ensure that notation matches prev chapter}

\parencite{barber2012bayesian,jordan1998learning}
Variational Bayes (VB) is a fully Bayesian methodology where one seeks
for an approximation to the parameter posterior
\begin{align}
	\Pdf{\Th}{\Y}\propto \defint{\mathcal{X}}{}{\Pdf{\X,\Y}{\Th}}{\X}\Pdf{\Th}	
\end{align}
As mentioned earlier, finding this distribution is commonly intractable, so in VB
we assume a factorized form for the joint posterior of states and parameters
\begin{align}
	\Pdf{\X,\Th}{\Y}\approx \Pdf[q]{\X}\Pdf[q]{\Th}
	\label{eq:VB_factorization}
\end{align}
and the task is then to find the best approximation with respect
to the KL divergence between the true posterior and the approximation
\begin{align}
	\KL{\Pdf[q]{\X}\Pdf[q]{\Th}}{\Pdf{\X,\Th}{\Y}} &= \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} + \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} -
	\E{\Pdf{\X,\Th}{\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}}.
	\label{eq:KL_VB}
\end{align}
Using ${\Pdf{\X,\Th}{\Y}}=\Pdf{\X,\Th,\Y}/\Pdf{\Y}$ equation \eqref{eq:KL_VB} gives
\begin{align}
	\lLH &\geq \E{\Pdf{\X,\Th,\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}} - \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} -
	\E{\Pdf[q]{\X}}{\Pdf[q]{\X}}
	\label{eq:VB_bound}
\end{align}
and thus minimizing the KL divergence is equivalent to finding the tightest lower bound to
the log likelihood. Analogously to EM, minimizing the KL divergence is done iteratively
keeping $\Pdf[q]{\Th}$ fixed and minimizing w.r.t $\Pdf[q]{\X}$ in the ``E''-step
and vice versa in the ``M''-step:

\begin{description}
\addtolength{\leftskip}{1cm}
\item[E-step]
\begin{align}
	\Pdf[q^{\text{new}}]{\X}=\argmax_{\Pdf[q]{\X}}\KL{\Pdf[q]{\X}\Pdf[q^{\text{old}}]{\Th}}{\Pdf{\X,\Th}{\Y}}
	\label{eq:VB_E}
\end{align}
\item[M-step]
\begin{align}
	\Pdf[q^{\text{new}}]{\Th}=\argmax_{\Pdf[q]{\Th}}\KL{\Pdf[q^{\text{new}}]{\X}\Pdf[q]{\Th}}{\Pdf{\X,\Th}{\Y}}
	\label{eq:VB_M}
\end{align}
\end{description}


Let us then suppose that we only wish to find the MAP point estimate $\Th^*$. This can be accomplished
by assuming a delta function form $\Pdf[q]{\Th}=\delta\left(\Th,\Th^*\right)$ for the parameter factor in the
joint distribution of states and parameters \eqref{eq:VB_factorization}.
With this assumption equation \eqref{eq:VB_bound} becomes
\begin{align}
	\Pdf{\Y}{\Th^*} &\geq \E{\Pdf{\X,\Th^*,\Y}}{\Pdf[q]{\X}\Pdf[q]{\Th}} - \E{\Pdf[q]{\X}}{\Pdf[q]{\X}} + \mathtt{const}
	\label{eq:VB_MAP_boundl}
\end{align}
and the ``M''-step \eqref{eq:VB_M} can then be written as
\begin{align}
	\Th^* &= \argmax_{\Th}\brak*{\E{\log\cLH}{\Pdf[q]{\X}}+\log\Pdf{\Th}}.	
\end{align}
If the point estimate is plugged in the ``E''-step equation \eqref{eq:VB_E} we have
\begin{align}
	\Pdf[q^{\text{new}}]{\X}\propto \Pdf{\X,\Y}{\Th^*} \propto \Pdf{\X}{\Y,\Th^*} 	
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Partial E and M steps}%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\parencite{Lange1995,Goodwin2005}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{EM in SSMs}%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:EM_SSM}

Let us then look at how to apply EM to a SSM of the form \eqref{eq:ssm_general}. First of
all, from the factorization in \eqref{eq:complete_data_likelihood}, the complete-data log-likelihood becomes
\begin{align*}
\begin{split}
	\cLH =&-\frac{1}{2}\left(\x_0-\gv{\mu}_0\right)^\tr\gv{\Sigma}_0^{-1}\left(\x_0-\gv{\mu}_0\right)-\frac{1}{2}\log\abbs{\gv{\Sigma}_0}\\
	&-\frac{1}{2}\sum_{k=1}^T\left(\x_k-\v{f}(\x_{k-1})\right)^\tr\v{Q}^{-1}\left(\x_k-\v{f}(\x_{k-1})\right)-\frac{T}{2}\log\abbs{\v{Q}}\\
	&-\frac{1}{2}\sum_{k=1}^T\left(\y_k-\v{h}(\x_{k})\right)^\tr\v{R}^{-1}\left(\y_k-\v{h}(\x_{k})\right)-\frac{T}{2}\log\abbs{\v{R}}\\
	&+\mathtt{const}
\end{split}
\end{align*}
Taking the expectation w.r.t. $\Pdf{\X}{\Y,\Th'}$ (assumed implicitly in the notation), applying the identity $\v{a}^\tr\v{C}\v{b}=\Tr{\v{a}^\tr\v{C}\v{b}}=\Tr{\v{C}\v{b}\v{a}^\tr}$, 
denoting $\f_{k-1}\equiv \f(\x_{k-1})$ and $\h_k\equiv \h(\x_k)$ and dropping the constant terms we get
\begin{align}
\begin{split}
	\EMQ{\Th'}{\Th} =&-\frac{1}{2}\Tr{\gv{\Sigma}_0^{-1}\E{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^\tr}}-\frac{1}{2}\log\abbs{\gv{\Sigma}_0}\\
	&-\frac{1}{2}\Tr[\bigg]{\v{Q}^{-1}\sum_{k=1}^T\E{\left(\x_k-\f_{k-1}\right)\left(\x_k-\f_{k-1}\right)^\tr}}-\frac{T}{2}\log\abbs{\v{Q}}\\
	&-\frac{1}{2}\Tr[\bigg]{\v{R}^{-1}\sum_{k=1}^T\E{\left(\y_k-\h_{k}\right)\left(\y_k-\h_{k}\right)^\tr}}-\frac{T}{2}\log\abbs{\v{R}}
\end{split}
\label{eq:eclLH}
\end{align} 
Let us denote the three expectations in equation~\eqref{eq:eclLH} with
\begin{align}
	\v{I}_1 &= \E{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^\tr}\\ 
	&=\defint{\mathcal{X}\times T}{}{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^\tr\Pdf{\X}{\Y,\Th'}}{\X}\nonumber\\
	&= 	\defint{\mathcal{X}}{}{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^\tr\Pdf{\x_0}{\Y,\Th'}}{\x_0} \label{eq:I1_general}\\
	\v{I}_2 &= \sum_{k=1}^T\defint{\mathcal{X}}{}{\left(\x_k-\f_{k-1}\right)\left(\x_k-\f_{k-1}\right)^\tr\Pdf{\xk,\xkk}{\Y,\Th'}}{\xk}{\xkk} \label{eq:I2_general}\\
	\v{I}_3 &= \sum_{k=1}^T\defint{\mathcal{X}}{}{\left(\y_k-\h_{k}\right)\left(\y_k-\h_{k}\right)^\tr\Pdf{\xk}{\Y,\Th'}}{\xk} \label{eq:I3_general}
\end{align}
It is clear then that in the E-step one needs to compute the $T+1$ smoothing
distributions, including the $T$ cross-timestep distributions, since these
will be needed in the expectations.
By applying the identity
\begin{align}
	\var{\x}&=\E{\x\x^\tr}-\E{\x}\E{\x}^\tr,
\end{align} 
we can already write the first expectation as
\begin{align}
	\v{I}_1&= \v{P}_{0|T}+(\v{m}_{0|T}-\gv{\mu}_0)(\v{m}_{0|T}-\gv{\mu}_0)^\tr.
	\label{eq:I1}
\end{align}
This was a result of assuming the Gaussian prior distribution of equation~\eqref{eq:prior} and
it is common for the more specific models we're assessing in the next chapters.

Since in this thesis we're assuming Gaussian noise, the M-step maximization equations for
the complete noise covariance matrices $\v{Q}$ and $\v{R}$ will also be common.
To derive these equations, we will derivate \eqref{eq:eclLH} with respect to these
matrices. As can be seen from \eqref{eq:eclLH}, the terms involving $\v{Q}$ or $\v{R}$
are analogous and so we will only the derivation for $\v{Q}$ and only the result
for $\v{R}$. It's easier to take the derivative with respect to $\v{Q}^{-1}$:
\begin{align}
	\dpd{\EMQ{\Th'}{\Th}}{\v{Q^{-1}}}=
	&-\frac{1}{2}\dpd{}{\v{Q^{-1}}}\Tr[\bigg]{\v{Q}^{-1}\sum_{k=1}^T\E{\left(\xk-\f_{k-1})\right)\left(\xk-\f_{k-1})\right)^\tr}}\nonumber\\
	&-\frac{T}{2}\dpd{}{\v{Q^{-1}}}\log\abbs{\v{Q}}\nonumber\\
	=&-\frac{1}{2}\sum_{k=1}^T\E{\left(\xk-\f_{k-1})\right)\left(\xk-\f_{k-1})\right)^\tr}+\frac{T}{2}\v{Q},
	\label{eq:clLH_pdQ}
\end{align}
where we have used formula 92 in \cite{Petersen2008} for the first derivative and 
formula 51 for the second. Setting \eqref{eq:clLH_pdQ}  
to zero \todo{Why is it same for the inverse?} we get the update equation for the next estimate of $\v{Q}$
\begin{align}
	\v{Q}_{j+1}&=\frac{1}{T}\sum_{k=1}^T\E{\left(\xk-\f_{k-1}\right)\left(\xk-\f_{k-1}\right)^\tr} \label{eq:EM_M_Q}\\
	&=\frac{1}{T}\v{I}_2\nonumber
\end{align}
The derivation of the update equation for the next estimate of $\v{R}$ is exactly analogous, giving
\begin{align}
	\v{R}_{j+1}&=\frac{1}{T}\sum_{k=1}^T\E{\left(\yk-\h_{k}\right)\left(\yk-\h_{k}\right)^\tr} \label{eq:EM_M_R}\\
	&=\frac{1}{T}\v{I}_3\nonumber
\end{align} 
In the following more specific cases, we will only
consider the two remaining expectations and the rest of the M-step maximization
equations.

Let us then discuss a certain important aspect of the M-step maximization equations, with a consequence
that is useful also for gradient based methods for ML. Often times
one is not interested in estimating any \emph{full} matrix, with which we mean that the model
might have been parameterized so that there are only scalar parameters embedded inside matrices or 
as part of $\f$ or $\h$. In that case
it is quite likely that the M-step maximization equations will not admit themselves to closed form
analytical expressions. Rather the use of nonlinear optimization methods is needed and for that
we would like have the gradient of $\EMQ$. This is fortunately rather straightforward. Let
$\theta_i$ be an element of $\Th$. Then
\begin{align}
\label{eq:dLB_nonlinear}
\begin{split}
	\dpd{\EMQ{\Th}{\Th'}}{\theta_i}
	=\quad{}&\frac{1}{2}\mathrm{Tr}\!\Bigg[\gv{\Sigma}^{-1}\bigg(
	\dpd{\gv{\Sigma}}{\theta_i}\gv{\Sigma}^{-1}
	\sum_{k=1}^T\E{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^\tr}{\hat{\Th}_j}\\
%
	&\quad+2\sum_{k=1}^T\E[\Big]{\dpd{\gv{\mu}_0}{\theta_i}\left(\x_0-\gv{\mu}_0\right)^\tr}
	-\dpd{\gv{\Sigma}}{\theta_i}\bigg)\Bigg]\\
	+&\frac{1}{2}\mathrm{Tr}\!\Bigg[\v{Q}^{-1}\bigg(\dpd{\v{Q}}{\theta_i}\v{Q}^{-1}
	\sum_{k=1}^T\E{\left(\x_k-\f_{k-1}\right)\left(\x_k-\f_{k-1}\right)^\tr}\\
%	
	&\quad +2\sum_{k=1}^T\E[\Big]{\dpd{\f_{k-1}}{\theta_i}\left(\x_k-\f_{k-1}\right)^\tr}
	-T\dpd{\v{Q}}{\theta_i}\bigg)\Bigg]\\
	+&\frac{1}{2}\mathrm{Tr}\!\Bigg[\v{R}^{-1}\bigg(\dpd{\v{R}}{\theta_i}\v{R}^{-1}
	\sum_{k=1}^T\E{\left(\y_k-\h_{k}\right)\left(\y_k-\h_{k}\right)^\tr}\\
	&\quad +2\sum_{k=1}^T\E[\Big]{\dpd{\h_{k}}{\theta_i}\left(\y_k-\h_{k}\right)^\tr}
	-T\dpd{\v{R}}{\theta_i}\bigg)\Bigg].
\end{split}	
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EM in linear-Gaussian SSMs}%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\parencite{shumway1982approach,Ghahramani1996}
Let us substitute $\v{A}\xkk$ for $\f_{k-1}$ and $\v{H}\xk$ for $\h_{k}$.
Let us also denote by
\begin{align}
	\Pdf{\xk, \xkk}{\v{Y},\Th}&=
	\N[
	\begin{bmatrix}
		\xk\\\xkk
	\end{bmatrix}
	]{\m_{k,k-1|T}}{\P_{k,k-1|T}} \label{eq:pdf_smooth_joint},
\end{align}
the joint smoothing distribution of $\xk$ and $\xkk$.
Then by applying the manipulation
\begin{align}
\begin{split}
&\E{\left(\v{x}_k-\v{A}\v{x}_{k-1}\right)\left(\v{x}_k-\v{A}\v{x}_{k-1}\right)^\tr}\\
=&\bm{\v{I} & -\v{A}}	
\E{
\begin{bmatrix}
	\xk\\\xkk
\end{bmatrix}
\begin{bmatrix}
	\xk^\tr & \xkk^\tr	
\end{bmatrix}
}
\bm{\v{I}\\-\v{A}^\tr}	
\end{split}
\end{align}
we get
\begin{align}
	\v{I}_{2}&=
\bm{\v{I} & -\v{A}}	
\sum_{k=1}^T\left(\P_{k,k-1|T}+\m_{k,k-1|T}\m_{k,k-1|T}^\tr\right)
\bm{\v{I}\\-\v{A}^\tr} \label{eq:LGSSM_I2}\\
&=\bm{\v{I} & -\v{A}}	
%\bm{\P_{k|T}+\m_{k|T}\m_{k|T}^\tr & \left(\P_{k,k-1|T}+\m_{k,k-1|T}\m_{k,k-1|T}^\tr\right)^\tr \\ \P_{k,k-1|T}+\m_{k,k-1|T}\m_{k,k-1|T}^\tr & \P_{k-1|T}+\m_{k-1|T}\m_{k-1|T}^\tr }
\bm{\sum_{k=1}^T\E{\xk\xk^\tr} & \sum_{k=1}^T\E{\xk\xkk^\tr} \\ \sum_{k=1}^T\E{\xkk\xk^\tr} & \sum_{k=1}^T\E{\xkk\xkk^\tr} }
\bm{\v{I}\\-\v{A}^\tr}\nonumber\\
&=\bm{\v{I} & -\v{A}}	
\bm{\bar{\X}_{00} & \bar{\X}_{01} \\ \bar{\X}_{01}^\tr & \bar{\X}_{11} }
\bm{\v{I}\\-\v{A}^\tr}\nonumber\\
&=\bar{\X}_{00}-\v{A}\bar{\X}_{01}^\tr-\bar{\X}_{01}\v{A}^\tr+\v{A}\bar{\X}_{11}\v{A}^\tr\nonumber\\
&=\left(\v{A}-\bar{\X}_{01}\bar{\X}_{11}^{-1}\right)\bar{\X}_{11}\left(\v{A}-\bar{\X}_{01}\bar{\X}_{11}^{-1}\right)^\tr+\bar{\X}_{00}+\bar{\X}_{01}\bar{\X}_{11}^{-1}\bar{\X}_{01}^\tr.
\label{eq:Amax}
\end{align}
It's easy to see that the extremum value of the last line with respect to $\v{A}$
is obtained by setting
\begin{align}
	\v{A}_{j+1}&=\bar{\X}_{01}\bar{\X}_{11}^{-1} \label{eq:EM_M_A}.	
\end{align}
For $\v{I}_3$ we get
\begin{align}
\v{I}_{3}&=
\bm{\v{I} & -\v{H}}	
\sum_{k=1}^T
\bm{\yk\yk^\tr & \yk\E{\xk}^\tr \\ \E{\xk}\yk^\tr & \E{\xk\xk^\tr} }
\bm{\v{I}\\-\v{H}^\tr} \label{eq:LGSSM_I3}\\
&=\bm{\v{I} & -\v{H}}	
\sum_{k=1}^T
	\bm{\bar{\Y}_{00} & \bar{\v{C}}_{00} \\ \bar{\v{C}}_{00}^\tr & \bar{\X}_{00} }
\bm{\v{I}\\-\v{H}^\tr},
\end{align}
so with analogous manipulation as in \eqref{eq:Amax} we get
\begin{align}
	\v{H}_{j+1}&=\bar{\v{C}}_{00}\bar{\X}_{00}^{-1} \label{eq:EM_M_H}.	
\end{align}

All in all, the E-step of EM algorithm in linear-Gaussian SSM:s consists of
computing the $T$ joint distributions in equation~\eqref{eq:pdf_smooth_joint} with the RTS smoother.
Actually this is not the only option, as for example in
\textcite{Elliott1999} a new kind of filter is presented that
can compute the expectations with only forward recursions.
After this, the M-step estimates are computed for $\v{Q}$
from equation~\eqref{eq:EM_M_Q}, for $\v{R}$ from equation~\eqref{eq:EM_M_R}, for $\v{A}$ from equation~\eqref{eq:EM_M_A}
and for $\v{H}$ from equation~\eqref{eq:EM_M_H}. Also for evaluating the convergence,
one needs to compare the current value of $\EMQ$ with the previous one.
For this we can compute $\v{I}_1$ from equation~\eqref{eq:I1},
$\v{I}_2$ from equation~\eqref{eq:LGSSM_I2} and $\v{I}_3$\todo{index $\v{I}$:s} from equation~\eqref{eq:LGSSM_I3}.

For working with structured matrices let us rewrite the gradient equation~\eqref{eq:dLB_nonlinear}
in the linear-Gaussian case:

\begin{align}
\label{eq:dLB_linear}
\begin{split}
	\dpd{\EMQ{\Th}{\Th'}}{\theta_i}
	=\quad{}&\frac{1}{2}\mathrm{Tr}\!\Bigg[\gv{\Sigma}^{-1}\bigg(
	\dpd{\gv{\Sigma}}{\theta_i}\gv{\Sigma}^{-1}
	\sum_{k=1}^T\E{\left(\x_0-\gv{\mu}_0\right)\left(\x_0-\gv{\mu}_0\right)^\tr}{\hat{\Th}_j}\\
	&\quad+2\sum_{k=1}^T\E[\Big]{\dpd{\gv{\mu}_0}{\theta_i}\left(\x_0-\gv{\mu}_0\right)^\tr}
	-\dpd{\gv{\Sigma}}{\theta_i}\bigg)\Bigg]\\
%
	+&\frac{1}{2}\mathrm{Tr}\!\Bigg[\v{Q}^{-1}\bigg(\dpd{\v{Q}}{\theta_i}\v{Q}^{-1}
	\bm{\v{I} & -\v{A}}\bm{\bar{\X}_{00} & \bar{\X}_{01} \\ \bar{\X}_{01}^\tr & \bar{\X}_{11} }\bm{\v{I}\\-\v{A}^\tr}\\	
	&\quad +2\bm{\v{0} & -\dpd{\v{A}}{\theta_i}}\bm{\bar{\X}_{00} & \bar{\X}_{01} \\ \bar{\X}_{01}^\tr & \bar{\X}_{11} }\bm{\v{I}\\-\v{A}^\tr}
	-T\dpd{\v{Q}}{\theta_i}\bigg)\Bigg]\\
%	
	+&\frac{1}{2}\mathrm{Tr}\!\Bigg[\v{R}^{-1}\bigg(\dpd{\v{R}}{\theta_i}\v{R}^{-1}
	\bm{\v{I} & -\v{H}}\bm{\bar{\Y}_{00} & \bar{\v{C}}_{00} \\ \bar{\v{C}}_{00}^\tr & \bar{\X}_{00} }\bm{\v{I}\\-\v{H}^\tr}\\
	&\quad +2\bm{\v{0} & -\dpd{\v{H}}{\theta_i}}\bm{\bar{\Y}_{00} & \bar{\v{C}}_{00} \\ \bar{\v{C}}_{00}^\tr & \bar{\X}_{00} }\bm{\v{I}\\-\v{H}^\tr}
	-T\dpd{\v{R}}{\theta_i}\bigg)\Bigg].
\end{split}	
\end{align}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EM in nonlinear-Gaussian SSMs}%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As explained in section \ref{sec:nonlinear_state}, in then nonlinear case
the filtering and smoothing distributions cannot be computed exactly.
Thus the E-step is also only approximate and the convergence
guarantees of EM won't apply anymore. The proposed methods can nevertheless
provide good results most of the time. In the fortunate case that the
model is linear-in-the-parameters the M-step can be solved in closed form.
This situation will be covered later in section~\ref{sec:litp}. Currently we will assume
however that the model is nonlinear in the parameters as well as in the states so that
the simplest form to write the model is exactly \eqref{eq:ssm_general}.

The E-step in the EM-algorithm consists now of computing approximations to $\v{I}_1$, $\v{I}_2$ and $\v{I}_3$
from equations~\eqref{eq:I1}, \eqref{eq:I2_general} and \eqref{eq:I3_general}. In the
M-step one should a gradient based nonlinear optimization algorithm to compute
the next parameter estimate \eqref{eq:EM_M}. The gradient can be computed with
the equation~\eqref{eq:dLB_nonlinear}.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{EM in linear-in-the-parameters SSM:s}%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:litp}

Suppose now that $\v{f}(\x):\mathcal{X}\to\mathcal{X}$ is a linear
combination of vector valued functions $\gv{\rho}_k:\mathcal{X}\to\R^{d_{\Phi,k}}$,
so that the parameters of $\f$, $\gv{\Phi}_k$, are matrices of size $d_x\times d_{\Phi,k}$.
Then $\f$ can be written as 
\begin{align}
\begin{split}
	\v{f}(\x)&=\gv{\Phi}\gv{\rho}_1(\x)+\dots+\gv{\Phi}_m\gv{\rho}_m(\x)\\
	&=
	\begin{bmatrix}
		\gv{\Phi}_1 & \dots & \gv{\Phi}_m
	\end{bmatrix}
	\begin{bmatrix}
		\gv{\rho}_1(\x)\\
		\vdots\\ 
		\gv{\rho}_m(\x)
	\end{bmatrix}\\
	&=\v{A}\v{g}(\x),
\end{split}
\end{align}
so that $\v{A}$ is now a matrix of size ${d_x\times\sum_{k=1}^m d_{\Phi,k}}$ and $\v{g}:\mathcal{X}\to \R^{\sum_{k=1}^m d_{\Phi,k}}$ . 
Denoting $\v{g}\left(\xkk\right)\equiv\v{g}_{k-1}$ and $\v{l}\left(\xk\right)\equiv\v{l}_{k}$
and following the derivation in equation~\eqref{eq:LGSSM_I2} we have
\begin{align}
	\v{I}_{2}
	&=\bm{\v{I} & -\v{A}}	
	\bm{\sum_{k=1}^T\E{\xk\xk^\tr} & \sum_{k=1}^T\E{\xk\v{g}_{k-1}^\tr} \\ \sum_{k=1}^T\E{\v{g}_{k-1}\xk^\tr} & \sum_{k=1}^T\E{\v{g}_{k-1}\v{g}_{k-1}^\tr} }
	\bm{\v{I}\\-\v{A}^\tr}\nonumber\\
	&=\bm{\v{I} & -\v{A}}	
	\bm{\bar{\X}_{00} & \bar{\v{G}}_{01} \\ \bar{\v{G}}_{01}^\tr & \bar{\v{G}}_{11} }
	\bm{\v{I}\\-\v{A}^\tr}
\label{eq:I2_LIP}
\end{align}
Then similarly to \eqref{eq:EM_M_A}
\begin{align}
	\v{A}_{j+1}&=\bar{\v{G}}_{01}\bar{\v{G}}_{11}^{-1} \label{eq:EM_LIP_M_A}.	
\end{align}

Applying similar assumptions as for $\v{f}$ to $\h$, we can write
\begin{align}
\begin{split}
	\v{h}(\x)&=\gv{\Upsilon}\gv{\pi}_1(\x)+\dots+\gv{\Upsilon}_m\gv{\pi}_m(\x)\\
	&=
	\begin{bmatrix}
		\gv{\Upsilon}_1 & \dots & \gv{\Upsilon}_m
	\end{bmatrix}
	\begin{bmatrix}
		\gv{\pi}_1(\x)\\
		\vdots\\ 
		\gv{\pi}_m(\x)
	\end{bmatrix}\\
	&=\v{H}\v{l}(\x),
\end{split}
\end{align}
where the size of $\v{H}$ is now ${d_x\times\sum_{k=1}^m d_{\Upsilon,k}}$ and 
$\v{l}:\mathcal{X}\to \R^{\sum_{k=1}^m d_{\Upsilon,k}}$ .
For $\v{I}_3$ we get
\begin{align}
	\v{I}_{3}&=
	\bm{\v{I} & -\v{H}}	
	\sum_{k=1}^T
	\bm{\yk\yk^\tr & \yk\E{\v{l}_k}^\tr \\ \E{\v{l}_k}\yk^\tr & \E{\v{l}_k\v{l}_k^\tr} }
	\bm{\v{I}\\-\v{H}^\tr} \label{eq:I3_LIP}\\
	&=\bm{\v{I} & -\v{H}}	
	\sum_{k=1}^T
		\bm{\bar{\Y}_{00} & \bar{\v{D}}_{00} \\ \bar{\v{D}}_{00}^\tr & \bar{\v{L}}_{00} }
	\bm{\v{I}\\-\v{H}^\tr} \nonumber,
\end{align}
and
\begin{align}
	\v{H}_{j+1}&=\bar{\v{D}}_{00}\bar{\v{L}}_{00}^{-1} \label{eq:EM_LIP_M_H}.	
\end{align}


\subsubsection{Convergence properties}

Let us denote by $\EMM$ the point-to-set map implicitly defined by the EM
algorithm and by $\brac*{\Th_j}$ the sequence of parameter estimates
\begin{align}
	 \Th_{j+1} &\in \EMM{\Th_{j}},
\end{align}
given the initial point $\Th_0$.

Suppose now that the set $\Theta^0=\brac{\Th\in\Theta : \lLH[\Th]\geq \Th_0}$ 
is compact and contained in the interior of $\Theta$ for any $\Th^0$,
the solution set for the EM algorithm initialized at $\Th_0$ is 
$\mathcal{L}^0=\brac*{\Th\in\Theta^0 : \dif\lLH[\Th]=0}$, 
that $\brac*{\Th_j}$ is contained in a compact subset of $\Theta$,
$\EMH$ is continuous in both of its arguments and
$\lLH[\Th]\geq\lLH[\Th']\quad \forall \Th\in\EMM{\Th'}$. 
Then $\lim_{j\to\infty}\Th_j\in\mathcal{L}^0$ and 
this also applies to every subsequence of $\brac*{\Th_j}$. Additionally
$\lim_{j\to\infty}\lLH[\Th_j]=\lLH[\Th_\star]$ where $\Th_\star\in\mathcal{L}^0$.
It is important to realize that the convergence of $\lLH[\Th_j]$ does not imply
that the sequence of parameters itself converges, only that the points in $\mathcal{L}^0$
are equivalent with respect to the objective function. This ties in with the the questions
of identifiability discussed earlier. In order for the parameter sequence to converge to some 
$\Th_\star\in\mathcal{L}^0$, we need to assume in addition that $\lim_{j\to\infty}\abs{\Th_{j+1}-\Th_{j}}=0$.

We will now assume that $\EMM$ is singular, i.e.
$\v{M}:\Theta\to\Theta$, and continuous. Then if $\brac[\big]{\Th_0,\EMM{\Th_0},\EMM{\EMM{\Th_0}},\dots }$
converges to a point $\Th_\star$, we must have $\Th_\star=\EMM{\Th_\star}$ and
thus around $\Th_\star$ we obtain the first order Taylor series expansion
\begin{align}
	\Th_{j+1}-\Th_\star&=\EMM{\Th_j}-\EMM{\Th_\star}\approx\dif\EMM{\Th_\star}\left(\Th_j-\Th_\star\right).
\end{align} 
We can then arrive at the following expression for $\dif\EMM{\Th_\star}$ \parencite{Dempster1977,Gibson2005,Lange1995}:
\begin{align}
	\dif\EMM{\Th_\star} &= \v{I}-\mathcal{I}_{\X\Y}^{-1}\mathcal{I}_{\Y}\\
	&=\mathcal{I}_{\X}\mathcal{I}_{\X\Y}^{-1}
	\label{eq:dM},
\end{align}
where
\begin{align}
		\mathcal{I}_{\X\Y} = \dQ &= \eval{\E{\prtdd{\log\cLH}{\Th}{\Th}}^{-1}_{\post{\Th_\star}}}_{\Th=\Th_\star}\\
		\mathcal{I}_{\Y} = \dL &= \eval{\E{\prtdd{\log\LH}{\Th}{\Th}}_{\post{\Th_\star}}}_{\Th=\Th_\star}\\
		\mathcal{I}_{\X} = \dH &= \eval{-\E{\prtdd{\log\post}{\Th}{\Th}}_{\post{\Th_\star}}}_{\Th=\Th_\star}
\end{align}
The latter term in equation~\eqref{eq:dM} is commonly interpreted as the proportion of ``measured'' information compared to
all information. All the eigenvalues of $\dif\EMM{\Th_\star}$ belong to $[0,1)$ and the \emph{linear} 
rate of convergence near $\Th_\star$ is the largest of them \parencite{Lange1995}. Then
clearly the rate of convergence is determined by how closely $\mathcal{I}_{\Y}$ ``resembles''
$\mathcal{I}_{\X\Y}$, i.e the more information is ``hidden'' in the hidden variables $\X$ the
slower is the convergence.\todo{Take Salakhutdinov2003 in here}   

There is still the problem that points in $\mathcal{L}^0$ can in addition
to being local maxima, be also saddle points or even local minima. In order to formulate
the conditions that guarantee that the stationary point is a local maximum, let us assume
from now on that the identifiability issues have been solved and $\EMM$ is thus a point-to-point map.
Then if $\mathcal{I}_{\X\Y}$ and $\mathcal{I}_{\X}$ are positive definite,
$\dif\EMM{\Th_\star}$ has only positive real eigenvalues and $\Th_\star$ is a
stable maximizer if and only if $\mathcal{I}_{\Y}$ has only negative eigenvalues.

\todo{What about Petersen2005?}
In \textcite{Salakhutdinov2003,Salakhutdinov2004} the transformation matrix $\F{\mathcal P}{\Th}$ is defined as
\begin{align}
	\Th_{j+1}-\Th_j &= \F{\mathcal P}{\Th_j}\nabla\lLH[\Th_j],
	\label{eq:EM_P}
\end{align}
then near $\Th_\star$ (i.e when $j$ is large)
we get
\begin{align}
		\F{\mathcal P}{\Th_j}&\approx -\left(\v{I}-\dif\EMM{\Th_j}\right)\dL^{-1}.
		\label{eq:EM_P_approx}
\end{align}
What we can infer from \eqref{eq:EM_P} and \eqref{eq:EM_P_approx} is that, near $\Th_\star$, if the dominant eigenvalue
of $\dif\EMM$ approaches unity, i.e. when the proportion of hidden information approaches zero, the EM algorithm behaves like 
a true Newton's method, where the step size is the negative inverse Hessian of the objective function. Also, as the dominant 
eigenvalue of $\dif\EMM$ approaches zero, i.e when the proportion of hidden information approaches unity, the convergence 
of the EM-algorithm becomes slower and stops. 


\parencite{Wu1983,Sandell1978,Meng1997,Elliott1999,Salakhutdinov2003a,Salakhutdinov2003,Olsson2007,Paninski2010}

\subsubsection{Computational load}

\parencite{Harvey1990,Watson1983,Cappe2005,Saatci2011,Olsson2007,Salakhutdinov2003a}


\subsection{Properties of the estimation algorithms}

Both the direct method of section~\ref{sec:grad} and the EM algorithm of section~\ref{sec:EM_SSM}
have their strengths and weaknesses. Neither of them can be said to eclipse the other in an absolute sense.
In this section we will go through some features of the said algorithms found in the literature.
Then a more detailed analysis will be performed on two essential aspects of any estimation algorithm:
the convergence properties, i.e. when should one expect the algorithm to find a local maximum and
the computational requirements of the algorithms.

\textcite{Cappe2005} contains a list of arguments in favor of either of the methods. The following list includes
those and additional points with comments:
\begin{description}
  \item[Direct]\hfill
\begin{itemize}
  \item\emph{No smoother needed} The log-likelihood, or an approximation to it, can be evaluated
  with forward-filtering
  \item\emph{No M-step}. There is no need to figure out model-dependent maximization
  formulas even in nonlinear models 
  \item\emph{Faster convergence}. Advanced gradient-based optimization
  methods can reach convergence speeds that are close to quadratic
\end{itemize}
  \item[EM]\hfill
  \begin{itemize}
  \item \emph{Simple to implement}. This argument is often put forward in favor of the EM
 algorithm. However in practice when using the gradient-based method one would use any one of
the off-the-self nonlinear optimizers and not re-implement one. Thus which one is easier to implement
boils down to gradient computation. If the model is linear or linear-in-the-parameters the EM
algorithm doesn't need any gradient information.
  \item\emph{Parameter constraints}. In \textcite{Cappe2005} it is argued that
since the M-step maximization equations are so simple, including parameter constraints
are easier in the EM algorithm. This again depends on if the model is linear or linear-in-the-parameters.
  \item\emph{Parameterization independent}. This again depends on if one has to use gradient-based
 optimization in the M-step. If not, then the EM algorithm is parameterization independent. In the gradient-based
 method the gradient and the Hessian, and so the convergence, are affected by the parameterization.
\end{itemize} 
\end{description}
 




\subsection{Linear-Gaussian State Space Models}

Since linear mappings can be described by matrices, stationary linear-Gaussian SSMs 
are described by the subset of SSMs of the form \eqref{eq:ssm_general} where  
\begin{align}
	\v{f}_{\Th}\left(\x_{k-1}\right)&=\v{A}\x_{k-1}\\
	\v{h}_{\Th}\left(\xk\right)&=\v{H}\xk
\end{align}
\todo{Explain something about linear-Gaussian SSMs}

\subsubsection{Kalman filter}

The recursions are as follows \parencite{Kalman1960,jazwinski2007stochastic}:
\todo{Elaborate on the Kalman filter}
\begin{subequations}
\label{eq:Kalman_filter}
\begin{description}
\addtolength{\leftskip}{1cm}
\item[Predict:]
\begin{align}
	\v{m}_{k|k-1}&=\v{A}\v{m}_{k-1|k-1}\\
	\v{P}_{k|k-1}&=\v{A}\v{P}_{k-1|k-1}\v{A}^\tr+\v{Q}
\end{align}
\item[Update:]
\begin{align}
	\v{v}_k&=\v{y}_k-\v{H}\v{m}_{k|k-1}\\
	\v{S}_k&=\v{H}\v{P}_{k|k-1}\v{H}^\tr+\v{R}\\
	\v{K}_k&=\v{P}_{k|k-1}\v{H}^\tr\v{S}_{k}^{-1}\\
	\v{m}_{k|k}&=\v{m}_{k|k-1}+\v{K}_k\v{v}_k\\
	\v{P}_{k|k}&=\v{P}_{k|k-1}-\v{K}_k\v{S}_k\v{K}_{k}^\tr
\end{align}
\end{description}
\end{subequations}
This includes the sufficient statistics for the $T$
joint distributions 
\begin{align}
	\Pdf{\v{x}_k,\v{y}_k}{\v{y}_{1:k-1},\gv{\theta}}
	&=\N[
	\begin{bmatrix}
		\v{x}_k\\\v{y}_{k}
	\end{bmatrix}
	]{
	\begin{bmatrix}
		\v{m}_{k|k-1}\\
		\v{H}\v{m}_{k|k-1}
	\end{bmatrix}
	}{
	\begin{bmatrix}
		\v{P}_{k|k-1} & \v{P}_{k|k-1}\v{H}^\tr\\
		\v{H}\v{P}_{k|k-1}^\tr & \v{S}_k  
	\end{bmatrix}
	}
	\label{eq:joint_per_kalmanstep}
\end{align}

\subsubsection{Rauch-Tung-Striebel Smoother}

The standard Rauch-Tung-Striebel (RTS) smoother gives the statistics $\v{m}_{k|T}$ and $\v{P}_{k|T}$ \parencite{jazwinski2007stochastic,Rauch1965}.
\begin{subequations}
\begin{align}
	\v{J}_k&=\v{P}_{k|k}\v{A}^\tr\v{P}_{k|k+1}^{-1}\\
	\v{m}_{k|T}&=\v{m}_{k|k}+\v{J}_k\left(\v{m}_{k+1|T}-\v{m}_{k+1|k}\right)\\
	\v{P}_{k|T}&=\v{P}_{k|k}+\v{J}_k\left(\v{P}_{k+1|T}-\v{P}_{k+1|k}\right)\v{J}_k^\tr
	%\v{C}_{k|T}&=\v{P}_{k|k}\v{J}_{k-1}^\tr+\v{J}_k\left(\v{C}_{k+1|T}-\v{A}\v{P}_{k|k}\right)\v{J}_{k-1}^\tr \label{eq:rts_cross_timestep_covariance}
\end{align}
\end{subequations}
\begin{align}
\begin{split} 
	\Pdf{\v{x}_k, \v{x}_{k-1}}{\v{Y},\Th}&=
	\N[\bm{\v{x}_k\\\v{x}_{k-1}}]{
	\bm{
		\v{m}_{k|T}\\
		\v{m}_{k-1|T}
	}
	}{
	\bm{
		\v{P}_{k|T} & \v{P}_{k|T}\v{J}_k^\tr\\
		\v{J}_k\v{P}_{k|T} & \v{P}_{k-1|T}  
	}
	}
\end{split}
\label{eq:sum_expectations}
\end{align}

In \parencite{Elliott1999} a new kind of filter is presented that
can compute \eqref{eq:sum_expectations} with only forward recursions. 
\todo{Elaborate on the RTS smoother}
\parencite{Paninski2010}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Nonlinear-Gaussian SSMs}%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:nonlinear_state}
In the nonlinear case at least one of the mappings $\v{f}_{\Th}$ and $\v{h}_{\Th}$ in
\eqref{eq:ssm_general} is nonlinear. Unfortunately in this case computing the filtering
distributions in closed form becomes intractable and one has to resort to 
some sort of approximations. Following \textcite{Arasaratnam2009}, we can
divide these approximate filtering (and smoothing) solutions into two 
categories: 
\begin{enumerate}[i)] \addtolength{\leftskip}{.5cm} \itemsep1pt \parskip0pt \parsep0pt
  \item \emph{Local approaches} assume the parametric form of the posterior
  distributions \eqref{eq:pred_bayes}, \eqref{eq:filt_bayes} and \eqref{eq:smooth_bayes} \emph{a priori}. 
  These  methods are analytically inexact but less computationally demanding. This is the category that
we will be concerned with in this thesis. 
  \item \emph{Global approaches} require the use of particle filtering (or sequential Monte Carlo), 
  which is asymptotically exact but computationally demanding
\end{enumerate}

The oldest and most well known filter belonging to the local approach is
the \emph{extended Kalman filter} (EKF) \parencite{jazwinski2007stochastic}, which is based 
on local linearisations.
In this way, i.e. by forming local linear approximations to the dynamic
and measurement models, the standard linear Kalman filter equations can be used.
An undesirable requirement of the EKF is that it requires computing
the Jacobian matrices of $\f$ and $\h$. A class of more recent deterministic approximation
based filters are known as \emph{sigma point filters}. 

\subsubsection{Gaussian filtering and smoothing}

One approach to forming Gaussian approximations is to assume a Gaussian
probability distribution a priori. Since a Gaussian distribution is 
defined by its first two moments, a moment matched approximation
can be obtained if the first two moments of the actual probability
distribution can be computed \parencite{Ito2000,Sarkka2006}. As will
be seen, computing these approximations reduces to the problem
of computing multidimensional integrals of the form \emph{nonlinear function} $\times$ \emph{Gaussian}.

Analogously to equation~\eqref{eq:ssm_distr_general}, let 
\begin{align}
	\v{a}&\sim \N{\m}{\gv{\Sigma}_a} \label{eq:pa}\\
	\v{b}|\v{a}&\sim \N{\F*{f}{\v{a}}}{\gv{\Sigma}_{b|a}} \label{eq:pb_given_a}
\end{align}
then 
\begin{align}
	\label{eq:joint_nongaussian}
	\Pdf{\v{a},\v{b}}&=\Pdf{\v{b}}{\v{a}}\Pdf{\v{a}}=\N[\v{a}]{\v{m}}{\gv{\Sigma}_a}\N[\v{b}]{\F*{f}{\v{a}}}{\gv{\Sigma}_{b|a}}
\end{align}
is only Gaussian if $\F*{f}{\v{a}}$ is linear. Assuming that's not the case,
let the Gaussian approximation
to \eqref{eq:joint_nongaussian} be

\begin{align}
	\label{eq:joint_gaussian_approx}
	\Pdf{\begin{bmatrix}
		\v{a}\\
		\v{b}
	\end{bmatrix}}&\approx
	\N{
	\begin{bmatrix}
		\gv{\mu}_a\\
		\gv{\mu}_b
	\end{bmatrix}
	}{
	\begin{bmatrix}
		\gv{\Sigma}_{aa}&\gv{\Sigma}_{ab}\\
		\gv{\Sigma}_{ab}^\tr&\gv{\Sigma}_{bb}
	\end{bmatrix}
	}
\end{align}
Then since the marginal distributions of a Gaussian distribution are also
Gaussian, we have to have
\begin{align}
	\gv{\mu}_a&=\v{m}&
	\gv{\Sigma}_{aa}&=\gv{\Sigma}_{a}\\
	\gv{\mu}_b&=\defint{}{}{\v{b}\,\Pdf{\v{b}}}{\v{b}} &
	\gv{\Sigma}_{bb}&=\defint{}{}{(\v{b}-\gv{\mu}_b)(\v{b}-\gv{\mu}_b)^\tr\Pdf{\v{b}}}{\v{b}} 
\end{align}
Both $\gv{\mu}_b$ and $\gv{\Sigma}_{bb}$ can be written in terms of \eqref{eq:pa} and \eqref{eq:pb_given_a}.
To see this, let us rewrite $\gv{\mu}_b$ as
\begin{align}
	\gv{\mu}_b&=\defint{}{}{\v{b}\,\Pdf{\v{b}}}{\v{b}} \nonumber\\
	&=\defint{}{}{\v{b}\Big(\!\defint{}{}{\Pdf{\v{b}}{\v{a}}\Pdf{\v{a}}}{\v{a}}\Big)}{\v{b}}\nonumber\\
	&\mbox{{\small (change the order of integration according to Fubini's theorem)}}\nonumber\\
	&=\defint{}{}{\v{f}(\v{a})\N{\v{m}}{\gv{\Sigma}_a}}{\v{a}}\label{eq:mean_int}
\end{align}
and $\gv{\Sigma}_{bb}$ as
\begin{align}
	\gv{\Sigma}_{bb}&=\defint{}{}{\v{b}\v{b}^\tr\Pdf{\v{b}}}{\v{b}}-\gv{\mu}_b\gv{\mu}_b^\tr \nonumber\\
	%&=\defint{}{}{\defint{}{}{\lbrack(\v{b}-\v{f}(\v{a}))(\v{b}-\v{f}(\v{a}))^\tr+\v{f}(\v{a})\v{f}(\v{a})^\tr\rbrack\Pdf{\v{b}}{\v{a}}}{\v{b}}\Pdf{\v{a}}}{\v{a}}-\gv{\mu}_b\gv{\mu}_b^\tr\\
	&=\defint{}{}{\v{f}(\v{a})\v{f}(\v{a})^\tr\Pdf{\v{a}}}{\v{a}}-\gv{\mu}_b\gv{\mu}_b^\tr
+\defint{}{}{\fparen*{\v{b}-\F*{f}{\v{a}}}\fparen*{\v{b}-\F*{f}{\v{a}}}^\tr\Pdf{\v{b}}{\v{a}}}{\v{a}}{\v{b}}\nonumber\\
	&=\defint{}{}{\fparen*{\F*{f}{\v{a}}-\gv{\mu}_b}\fparen*{\F*{f}{\v{a}}-\gv{\mu}_b}^\tr\N{\m}{\gv{\Sigma}_a}}{\v{a}}+\gv{\Sigma}_{b|a}\label{eq:var_int}.
\end{align}
Finally, the cross-covariance can be written as
\begin{align}
	\gv{\Sigma}_{ab}&=\defint{}{}{\fparen[\big]{\v{a}-\gv{\mu}_a}\fparen[\big]{\v{b}-\gv{\mu}_b}^\tr\Pdf{\v{a},\v{b}}}{\v{a}}{\v{b}} \nonumber\\
	&=\defint{}{}{\fparen[\big]{\v{a}-\gv{\mu}_a}\fparen[\big]{\v{b}-\gv{\mu}_b}^\tr\Pdf{\v{a}}\Pdf{\v{b}}{\v{a}}}{\v{a}}{\v{b}} \nonumber\\
	&=\defint{}{}{\fparen[\big]{\v{a}-\gv{\mu}_a}\fparen[\big]{\defint{}{}{\v{b}\,\Pdf{\v{b}}{\v{a}}}{\v{b}}-\gv{\mu}_b}^\tr\Pdf{\v{a}}}{\v{a}} \nonumber\\
	&=\defint{}{}{\fparen[\big]{\v{a}-\v{m}}\fparen*{\F*{f}{\v{a}}-\gv{\mu}_b}^\tr\N{\v{m}}{\gv{\Sigma}_a}}{\v{a}}\label{eq:cross_cov_int}
\end{align}
 
To see how this idea can be used to form a Gaussian
approximation to $\Pdf{\v{x}_{k-1},\v{x}_{k}}{\v{Y}}$, let us
rewrite it as
\begin{align}
\begin{split}
	\Pdf{\v{x}_{k-1},\v{x}_{k}}{\v{Y}}&=\Pdf{\v{x}_{k-1}}{\v{x}_{k},\v{Y}_{1:k-1}}\Pdf{\v{x}_{k}}{\v{Y}}\\
	&=\frac{\Pdf{\v{x}_{k-1},\v{x}_{k}}{\v{Y}_{1:k-1}}\Pdf{\v{x}_{k}}{\v{Y}}}{\Pdf{\v{x}_{k}}{\v{Y}_{1:k-1}}},
	\label{eq:joint_smooth}
\end{split}
\end{align}
where the dependence on the parameter $\Th$
is suppressed for clarity. Since the Gaussian approximation to
$\Pdf{\v{x}_{k-1},\v{x}_{k}}{\v{Y}}$ will be calculated by forward
(filtering) and backward (smoothing) recursions, let us assume that we already
have available the Gaussian approximation
\begin{align}
	\Pdf{\xkk}{\Y_{1:k-1}} &\approx \N[\xkk]{\m_{k-1|k-1}}{\P_{k-1|k-1}}.
\end{align}
Then
\begin{align}
	\Pdf{\xkk,\xk}{\v{Y}_{1:k-1}}&\approx\N[\xkk]{\m_{k-1|k-1}}{\P_{k-1|k-1}}\N[\xk]{\f_{k-1}}{\v{Q}}\\
	&\approx
	\N[\bm{\xkk \\ \xk}]{
	\begin{bmatrix}
		\v{m}_{k-1|k-1}\\
		\v{m}_{k|k-1}
	\end{bmatrix}}{
	\begin{bmatrix}
		\v{P}_{k-1|k-1}&\v{C}_{k^-}\\
		\v{C}_{k^-}^\tr&\v{P}_{k|k-1}
	\end{bmatrix}
	}
	\label{eq:joint_predictive_approximation}
\end{align}
where by application of equations \eqref{eq:mean_int}, \eqref{eq:var_int} and \eqref{eq:cross_cov_int} 
\begin{align}
	\begin{split}
	\v{m}_{k|k-1}&=\defint{}{}{\f_{k-1}\N[\xkk]{\m_{k-1|k-1}}{\P_{k-1|k-1}}}{\xkk}\label{eq:prediction_mean_intergral}
	\end{split}\\
	\begin{split}
	\v{P}_{k|k-1}&=\defint{}{}{\fparen*{\f_{k-1}-\v{m}_{k|k-1}}\fparen*{\f_{k-1}-\m_{k|k-1}}^\tr\\
	&\qquad\times\N[\xkk]{\m_{k-1|k-1}}{\P_{k-1|k-1}}}{\xkk}+\v{Q}\label{eq:prediction_variance_intergral}
	\end{split}\\
	\begin{split}
		\v{C}_{k^-}&=\defint{}{}{\fparen*{\xkk-\m_{k-1|k-1}}\fparen*{\f_{k-1}-\v{m}_{k|k-1}}^\tr\\
		&\qquad\times\N{\xkk}{\m_{k-1|T}}{\P_{k-1|T}}}{\xkk}\label{eq:prediction_cov_intergral}
	\end{split}
\end{align}
In order to calculate \eqref{eq:prediction_mean_intergral} and \eqref{eq:prediction_variance_intergral}
we also need a Gaussian approximation for the joint distribution of
the current state and measurement given the previous measurements
\begin{align}
\begin{split}
	\Pdf{\xk,\yk}{\Y_{1:k-1}}&\approx\N[\xk]{\m_{k|k-1}}{\P_{k|k-1}}\N[\yk]{\h_k}{\v{R}}\\
	&\approx 
	\N{
	\begin{bmatrix}
		\m_{k|k-1}\\
		\gv{\mu}_{k}
	\end{bmatrix}}{
	\begin{bmatrix}
		\v{P}_{k|k-1}&\v{C}_{k}\\
		\v{C}_{k}^\tr&\v{S}_{k}
	\end{bmatrix}
	}.
	\label{eq:joint_update_approximation}
\end{split}
\end{align}
Applying equations \eqref{eq:mean_int}, \eqref{eq:var_int} and \eqref{eq:cross_cov_int} again,
we get
\begin{align}
	\gv{\mu}_{k}
	&=\defint{}{}{\h_k\N[\xk]{\m_{k|k-1}}{\P_{k|k-1}}}{\xk}\label{eq:update_mean_intergral}\\
	\v{S}_{k}
	&=\defint{}{}{\fparen[\big]{\h_k-\gv{\mu}_k}\fparen[\big]{\h_k-\gv{\mu}_k}^\tr\N[\xk]{\m_{k|k-1}}{\P_{k|k-1}}}{\xk}+\v{R} \label{eq:update_variance_intergral}\\
	\v{C}_{k}
	&=\defint{}{}{\fparen[\big]{\xk-\m_{k|k-1}}\fparen[\big]{\h_k-\gv{\mu}_k}^\tr\N[\xk]{\m_{k|k-1}}{\P_{k|k-1}}}{\xk} \label{eq:update_covariance_intergral}
\end{align}
and by using the well known formula for calculating the conditional distribution of jointly Gaussian variables
we have
\begin{align}
	\m_{k|k}
	&=\m_{k|k-1}+\v{C}_{k}\v{S}_{k}^{-1}\left(\yk-\gv{\mu}_k\right)\label{eq:update_mean}\\
	\P_{k|k}
	&=\P_{k|k-1}-\v{C}_{k}\v{S}_{k}^{-1}\v{C}_{k}^\tr. \label{eq:update_variance}
\end{align}
Then finally we can write the Gaussian approximation to the joint distribution of consecutive states given all the measurements
as
\begin{align}
\begin{split}
	\Pdf{\xkk,\xk}{\v{Y}}&=\Pdf{\xkk}{\xk,\v{Y}_{1:k-1}}\Pdf{\xk}{\v{Y}}\\
	&\approx
	\N{
	\begin{bmatrix}
		\m_{k-1|T}\\
		\m_{k|T}
	\end{bmatrix}}{
	\begin{bmatrix}
		\P_{k-1|T}&\v{D}_{k}\\
		\v{D}_{k}^\tr&\P_{k|T}
	\end{bmatrix}
	}
\end{split}
\end{align}
where
\begin{align}
	\v{D}_{k}&=\v{G}_{k-1}\P_{k|T}\\
	\m_{k-1|T}&=\m_{k-1|k-1}+\v{G}_{k-1}\left(\m_{k|T}-\m_{k|k-1}\right)\\
	\P_{k-1|T}&=\P_{k-1|k-1}+\v{G}_{k-1}\left(\P_{k|T}-\P_{k|k-1}\right)\v{G}_{k-1}^\tr
\end{align}

What we have now established is that a Gaussian assumed density approximation to the
predictive distributions depends on solving the multidimensional integrals in equations
\eqref{eq:prediction_mean_intergral}, \eqref{eq:prediction_variance_intergral} and \eqref{eq:prediction_cov_intergral}.
Analogously, the filtering distribution approximations depend on solving the integrals
in equations \eqref{eq:update_mean_intergral}, \eqref{eq:update_variance_intergral} and
\eqref{eq:update_covariance_intergral}. All these integrals are of the same form,
namely 
\begin{align}
	\defint{}{}{&\F*{l}{\x}\N[\x]{\m}{\P}}{\x}\\
	\equiv\defint{}{}{&\F*{l}{\x}\fparen*{\fparen*{2\pi}^{d_x}\det\P}^{-\sfrac{1}{2}}\exp\brak[\big]{-\frac{1}{2}(\x-\m)^\tr\P^{-1}(\x-\m)}}{\x},
	\label{eq:gauss_integral}
\end{align}
with differing nonlinear functions $\F*{l}{\x}$. Notably, the smoothing distribution approximations can be computed
without further integrations.


\subsubsection{Quadrature and cubature}
 We will now discuss the topic of numerically solving integrals of the form \eqref{eq:gauss_integral}.
 Numerical integration in one dimension is known as \emph{quadrature} and in higher dimensions as \emph{cubature}.


\parencite{Arasaratnam2009,Ito2000}
\subsubsection{Unscented Kalman Filter and Smoother}
\parencite{julier1997new,Merwe2004}
\subsubsection{Cubature Kalman Filter and Smoother}
\parencite{Arasaratnam2009,Arasaratnam2011,Jia2012}



